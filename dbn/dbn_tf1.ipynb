{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600224544894",
   "display_name": "Python 3.8.5 64-bit ('tf2py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "success\n"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import tensorflow.keras as keras\n",
    "import math\n",
    "tf1.disable_eager_execution()\n",
    "class RBM(object):\n",
    "    def __init__(self,input_size,output_size,learning_rate=1.0):\n",
    "        self._input_size=input_size\n",
    "        self._output_size=output_size\n",
    "        self.w=np.zeros([input_size,output_size],np.float32)\n",
    "        self.hb=np.zeros([output_size],np.float32)\n",
    "        self.vb=np.zeros([input_size],np.float32)\n",
    "\n",
    "        self.learning_rate=learning_rate\n",
    "    # Fits the result from the weighted visible layer plus the bias into a sigmoid curve\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        # Sigmoid\n",
    "        return tf.math.sigmoid(tf.linalg.matmul(visible, w) + hb)\n",
    "     # Fits the result from the weighted hidden layer plus the bias into a sigmoid curve\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.math.sigmoid(tf.linalg.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    def sample_prob(self,probs):\n",
    "        return tf.nn.relu(tf.sign(probs-tf.random.uniform(tf.shape(probs))))\n",
    "    def train(self,X,epochs=2,batchsize=128):\n",
    "        _w=tf1.placeholder(\"float\",[self._input_size,self._output_size])\n",
    "        _hb=tf1.placeholder(\"float\",[self._output_size])\n",
    "        _vb=tf1.placeholder(\"float\",[self._input_size])\n",
    "\n",
    "        prv_w=np.zeros([self._input_size,self._output_size],np.float32)\n",
    "        prv_hb=np.zeros([self._output_size],np.float32)\n",
    "        prv_vb=np.zeros([self._input_size],np.float32)\n",
    "\n",
    "        cur_w=np.zeros([self._input_size,self._output_size],np.float32)\n",
    "        cur_hb=np.zeros([self._output_size],np.float32)\n",
    "        cur_vb=np.zeros([self._input_size],np.float32)\n",
    "        v0=tf1.placeholder(\"float\",[None,self._input_size])\n",
    "\n",
    "        h0=self.sample_prob(self.prob_h_given_v(v0,_w,_hb))\n",
    "        v1=self.sample_prob(self.prob_v_given_h(h0,_w,_vb))\n",
    "        h1=self.prob_h_given_v(v1,_w,_hb)\n",
    "\n",
    "        positive_grad=tf.linalg.matmul(tf.transpose(v0),h0)\n",
    "        negative_grad=tf.linalg.matmul(tf.transpose(v1),h1)\n",
    "\n",
    "        update_w=_w+self.learning_rate*(positive_grad-negative_grad)/tf.cast(tf.shape(v0)[0],dtype=tf.float32)\n",
    "        update_vb=_vb+self.learning_rate*tf.reduce_mean(v0-v1,0)\n",
    "        update_hb=_hb+self.learning_rate*tf.reduce_mean(h0-h1,0)\n",
    "\n",
    "        err=tf.reduce_mean(tf.square(v0-v1))\n",
    "\n",
    "        with tf1.Session() as sess:\n",
    "            sess.run(tf1.global_variables_initializer())\n",
    "            for epoch in range(epochs):\n",
    "                for start,end in zip(range(0,len(X),batchsize),range(batchsize,len(X),batchsize)):\n",
    "                    batch=X[start:end]\n",
    "                    cur_w  = sess.run(update_w,  feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w=cur_w\n",
    "                    prv_hb=cur_hb\n",
    "                    prv_vb=cur_vb\n",
    "                error=sess.run(err,feed_dict={v0:X,_w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "\n",
    "\n",
    "                print('Epoch: %d' % epoch, 'reconstruction error: %f' % error)\n",
    "\n",
    "            self.w=prv_w\n",
    "            self.hb=prv_hb\n",
    "            self.vb=prv_vb\n",
    "    \n",
    "    def rbm_outpt(self, X):\n",
    "        input_X = tf.constant(X,dtype=tf.float32)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        out = tf.math.sigmoid(tf.linalg.matmul(input_X, _w) + _hb)\n",
    "        with tf1.Session() as sess:\n",
    "            sess.run(tf1.global_variables_initializer())\n",
    "            return sess.run(out)\n",
    "\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "rbm=RBM(784,500)\n",
    "#print(train_images[0])\n",
    "\n",
    "train_images=train_images[:10000]\n",
    "train_images=train_images.reshape(-1,784)\n",
    "train_images=train_images/255.0\n",
    "#print(\"new:\",train_images[0])\n",
    "rbm.train(train_images)\n",
    "#print(rbm.rbm_outpt(train_images[:10]))\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RBM:  0   784 -> 500\nRBM:  1   500 -> 200\nRBM:  2   200 -> 50\nNew RBM:\nEpoch: 0 reconstruction error: 0.064032\nEpoch: 1 reconstruction error: 0.054998\nNew RBM:\nEpoch: 0 reconstruction error: 0.032073\nEpoch: 1 reconstruction error: 0.029116\nNew RBM:\nEpoch: 0 reconstruction error: 0.062018\nEpoch: 1 reconstruction error: 0.059181\n"
    }
   ],
   "source": [
    "\n",
    "'''训练每一个RBM'''\n",
    "RBM_hidden_sizes = [500, 200 , 50 ] #create 4 layers of RBM with size 785-500-200-50\n",
    "\n",
    "#Since we are training, set input as training data\n",
    "inpX = train_images\n",
    "\n",
    "#Create list to hold our RBMs\n",
    "rbm_list = []\n",
    "\n",
    "#Size of inputs is the number of inputs in the training set\n",
    "input_size = inpX.shape[1]\n",
    "#with tf.device('/gpu:0'):\n",
    "#For each RBM we want to generate\n",
    "for i, size in enumerate(RBM_hidden_sizes):\n",
    "    print('RBM: ',i,' ',input_size,'->', size)\n",
    "    rbm_list.append(RBM(input_size, size))\n",
    "    input_size = size\n",
    "#For each RBM in our list\n",
    "for rbm in rbm_list:\n",
    "    print('New RBM:')\n",
    "    #Train a new one\n",
    "    rbm.train(inpX) \n",
    "    #Return the output layer\n",
    "    inpX = rbm.rbm_outpt(inpX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "\n",
    "    def __init__(self, sizes, X, Y):\n",
    "        # Initialize hyperparameters\n",
    "        self._sizes = sizes\n",
    "        self._X = X\n",
    "        self._Y = Y\n",
    "        self.w_list = []\n",
    "        self.b_list = []\n",
    "        self._learning_rate = 1.0\n",
    "        self._momentum = 0.0\n",
    "        self._epoches = 10\n",
    "        self._batchsize = 100\n",
    "        input_size = X.shape[1]\n",
    "\n",
    "        # initialization loop\n",
    "        for size in self._sizes + [Y.shape[1]]:\n",
    "            # Define upper limit for the uniform distribution range\n",
    "            max_range = 4 * math.sqrt(6. / (input_size + size))\n",
    "\n",
    "            # Initialize weights through a random uniform distribution\n",
    "            self.w_list.append(\n",
    "                np.random.uniform(-max_range, max_range, [input_size, size]).astype(np.float32))\n",
    "\n",
    "            # Initialize bias as zeroes\n",
    "            self.b_list.append(np.zeros([size], np.float32))\n",
    "            input_size = size\n",
    "\n",
    "    # load data from rbm\n",
    "    def load_from_rbms(self, dbn_sizes, rbm_list):\n",
    "        # Check if expected sizes are correct\n",
    "        assert len(dbn_sizes) == len(self._sizes)\n",
    "\n",
    "        for i in range(len(self._sizes)):\n",
    "            # Check if for each RBN the expected sizes are correct\n",
    "            assert dbn_sizes[i] == self._sizes[i]\n",
    "\n",
    "        # If everything is correct, bring over the weights and biases\n",
    "        for i in range(len(self._sizes)):\n",
    "            self.w_list[i] = rbm_list[i].w\n",
    "            self.b_list[i] = rbm_list[i].hb\n",
    "\n",
    "    # Training method\n",
    "    def train(self):\n",
    "        # Create placeholders for input, weights, biases, output\n",
    "        _a = [None] * (len(self._sizes) + 2)\n",
    "        _w = [None] * (len(self._sizes) + 1)\n",
    "        _b = [None] * (len(self._sizes) + 1)\n",
    "        _a[0] = tf1.placeholder(\"float\", [None, self._X.shape[1]])\n",
    "        y = tf1.placeholder(\"float\", [None, self._Y.shape[1]])\n",
    "\n",
    "        # Define variables and activation functoin\n",
    "        for i in range(len(self._sizes) + 1):\n",
    "            _w[i] = tf1.Variable(self.w_list[i])\n",
    "            _b[i] = tf1.Variable(self.b_list[i])\n",
    "        for i in range(1, len(self._sizes) + 2):\n",
    "            _a[i] = tf.math.sigmoid(tf.linalg.matmul(_a[i - 1], _w[i - 1]) + _b[i - 1])\n",
    "\n",
    "        # Define the cost function\n",
    "        cost = tf.reduce_mean(tf.square(_a[-1] - y))\n",
    "\n",
    "        # Define the training operation (Momentum Optimizer minimizing the Cost function)\n",
    "        train_op = tf1.train.MomentumOptimizer(\n",
    "            self._learning_rate, self._momentum).minimize(cost)\n",
    "\n",
    "        # Prediction operation\n",
    "        predict_op = tf.argmax(_a[-1], 1)\n",
    "\n",
    "        # Training Loop\n",
    "        with tf1.Session() as sess:\n",
    "            # Initialize Variables\n",
    "            sess.run(tf1.global_variables_initializer())\n",
    "\n",
    "            # For each epoch\n",
    "            for i in range(self._epoches):\n",
    "\n",
    "                # For each step\n",
    "                for start, end in zip(\n",
    "                        range(0, len(self._X), self._batchsize), range(self._batchsize, len(self._X), self._batchsize)):\n",
    "                    # Run the training operation on the input data\n",
    "                    _,t=sess.run([train_op,_a[-1]], feed_dict={\n",
    "                        _a[0]: self._X[start:end], y: self._Y[start:end]})\n",
    "                    \n",
    "                    print(t[0])\n",
    "\n",
    "                for j in range(len(self._sizes) + 1):\n",
    "                    # Retrieve weights and biases\n",
    "                    self.w_list[j] = sess.run(_w[j])\n",
    "                    self.b_list[j] = sess.run(_b[j])\n",
    "\n",
    "                print(\"Accuracy rating for epoch \" + str(i) + \": \" + str(np.mean(np.argmax(self._Y, axis=1) == \\\n",
    "                                                                                 sess.run(predict_op, feed_dict={_a[0]: self._X, y: self._Y}))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(60000, 784)\n(60000, 10)\n"
    }
   ],
   "source": [
    "trX,trY=train_images, train_labels\n",
    "trY=np.eye(10)[trY]\n",
    "print(trX.shape)\n",
    "print(trY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.0868285  0.90751725 0.08484271 0.9544153  0.9938967  0.44257185\n 0.7776768  0.05140176 0.10552889 0.02157509]\n[0.02017495 0.9697535  0.5002643  0.03053227 0.99865806 0.3678844\n 0.74541277 0.11507311 0.20216629 0.00331405]\n[3.33907276e-01 9.98491049e-01 6.62581921e-01 2.37427056e-02\n 9.99677718e-01 1.94004178e-01 4.87455428e-02 1.19993776e-01\n 8.00967216e-04 6.60812855e-03]\n[0.24209821 0.9520881  0.02925718 0.18789262 0.99926174 0.03954968\n 0.4153376  0.42769602 0.15863627 0.00309423]\n[0.36314124 0.92600775 0.27958128 0.35154623 0.98658395 0.3309754\n 0.3153062  0.14191893 0.17149678 0.03592762]\n[0.00500798 0.9994228  0.7152697  0.04741058 0.99731815 0.13344416\n 0.46912003 0.01767513 0.11318865 0.03301612]\n[3.8594890e-01 9.5923138e-01 2.7563000e-01 6.2877238e-02 9.9971318e-01\n 1.1434668e-01 6.9195181e-02 3.7638044e-01 9.7668767e-03 5.6600571e-04]\n[0.03188288 0.23517692 0.09154212 0.04966021 0.99592316 0.12172368\n 0.02366111 0.0279755  0.24056792 0.00492084]\n[0.00238669 0.95873344 0.12212956 0.04946738 0.99836767 0.04052973\n 0.27632767 0.05103514 0.36260092 0.00618747]\n[3.9432096e-01 9.1874874e-01 1.3707024e-01 3.6069572e-02 9.9978304e-01\n 6.7601293e-02 4.3789595e-02 4.0981352e-01 1.5544891e-02 3.5744905e-04]\n[0.08751905 0.8369257  0.0043405  0.36330706 0.99055856 0.1296142\n 0.8390086  0.00824201 0.08497879 0.05315679]\n[0.30297655 0.95275104 0.01921475 0.27809387 0.9995364  0.24662706\n 0.11830014 0.0700976  0.01051763 0.0019944 ]\n[6.6927475e-01 9.8024464e-01 1.1758882e-01 1.0254383e-02 9.9850047e-01\n 3.0813128e-02 7.0803761e-03 1.0315594e-01 8.2364678e-04 4.9246252e-03]\n[0.01277435 0.8174169  0.10482222 0.01548705 0.99352086 0.00712571\n 0.00443307 0.00833979 0.04770797 0.01525974]\n[0.00207001 0.98359126 0.55298537 0.05646348 0.9782003  0.01761949\n 0.02111328 0.00281122 0.08764043 0.05284327]\n[4.1401297e-02 7.2373056e-01 9.4219744e-02 2.3331493e-02 9.9930429e-01\n 4.8882067e-03 1.3540685e-02 9.9967241e-02 7.4894637e-02 6.4012408e-04]\n[0.08076039 0.9909703  0.13735059 0.13004318 0.9806547  0.02543473\n 0.87729    0.01607281 0.0345889  0.01526278]\n[1.7056304e-01 9.9313319e-01 4.0575093e-01 1.1355132e-02 9.9955940e-01\n 4.5758158e-02 5.8656633e-03 8.1109732e-02 2.0387769e-04 2.7057528e-03]\n[0.16734728 0.6799619  0.14485371 0.11903748 0.78638947 0.44458786\n 0.18582353 0.1718924  0.03874213 0.3058847 ]\n[0.13176516 0.7747544  0.0508621  0.10377762 0.8738712  0.33464056\n 0.39189252 0.18384555 0.02569905 0.35267496]\n[0.02397656 0.79945767 0.07797676 0.01798847 0.7650777  0.12235817\n 0.3076043  0.03637016 0.48853028 0.13406375]\n[0.6079599  0.62687445 0.00768313 0.32844448 0.8940031  0.4260739\n 0.1822502  0.09680852 0.03882658 0.83129716]\n[0.0099777  0.8094518  0.16820851 0.27586007 0.9652331  0.40317208\n 0.49498716 0.18503642 0.6731491  0.06309238]\n[0.05446142 0.34642312 0.01900682 0.09337068 0.8578374  0.11579666\n 0.46919352 0.06382316 0.75488675 0.02946174]\n[0.00387454 0.8507898  0.1442743  0.12308738 0.9818197  0.13354278\n 0.08555639 0.02656603 0.13747212 0.04779828]\n[0.8897321  0.7823732  0.02425745 0.5718789  0.6959207  0.32670254\n 0.24954396 0.2581064  0.03237772 0.9461765 ]\n[1.3356867e-01 7.5154531e-01 1.5883079e-01 1.3220519e-02 9.9899423e-01\n 3.0831993e-02 1.8694967e-02 1.3435337e-01 7.3100030e-03 4.9328804e-04]\n[0.2500467  0.18634257 0.06294617 0.2955519  0.39181155 0.54403573\n 0.3847192  0.2061981  0.14511734 0.28075853]\n[1.2136996e-02 8.5341465e-01 7.9353303e-02 6.6417456e-03 9.9064022e-01\n 2.3774922e-02 1.8363729e-01 5.0342649e-02 6.3899845e-02 6.9111586e-04]\n[0.0236536  0.588051   0.06710356 0.03400752 0.9840152  0.16814977\n 0.02564681 0.03559867 0.03649148 0.01329347]\n[0.01610181 0.6158893  0.10430318 0.01950449 0.7510865  0.13657096\n 0.61521244 0.03216597 0.4766127  0.38552365]\n[0.0023275  0.52635014 0.03948936 0.03291923 0.9782723  0.02778068\n 0.20286357 0.03555822 0.28400713 0.00481224]\n[0.4313828  0.77100307 0.20796964 0.18975022 0.98333466 0.44793987\n 0.07537442 0.37847686 0.01841223 0.03916934]\n[0.16298589 0.14815009 0.09447935 0.26668105 0.18365416 0.31326312\n 0.5499764  0.20275596 0.3057521  0.27325362]\n[0.5382313  0.7118553  0.2554145  0.8157099  0.9827043  0.44979757\n 0.32972205 0.4578623  0.10649267 0.02675828]\n[0.06106701 0.16913724 0.02456108 0.03608784 0.98981774 0.01574549\n 0.07144046 0.05278307 0.04991013 0.00235873]\n[0.02020168 0.555727   0.04573891 0.00619176 0.95073783 0.11294153\n 0.28764307 0.12568083 0.13975173 0.00601617]\n[0.44683126 0.22509944 0.00999257 0.386219   0.18203488 0.05101287\n 0.04839557 0.07947525 0.4930705  0.3963009 ]\n[7.3739737e-02 9.7694647e-01 3.2357925e-01 7.2814524e-03 9.9850130e-01\n 3.3059239e-02 3.8056970e-03 7.7279508e-02 2.2509694e-04 2.1395683e-03]\n[8.4958971e-02 9.5801640e-01 2.5558454e-01 6.3060820e-03 9.9794269e-01\n 2.2650868e-02 2.1640956e-03 6.2228113e-02 1.3831258e-04 1.5565753e-03]\n[0.00601766 0.1333954  0.03767177 0.01477313 0.7983209  0.00670043\n 0.02198026 0.01600596 0.04980487 0.12471953]\n[0.00456849 0.09363014 0.01816782 0.02066612 0.7672068  0.03381956\n 0.03427356 0.03210825 0.21361586 0.07943919]\n[0.01733258 0.81052196 0.78187025 0.00778952 0.9929576  0.02879688\n 0.00576791 0.04980043 0.0135121  0.00260228]\n[0.29290956 0.8374295  0.03611636 0.00342005 0.9796145  0.02678108\n 0.00515917 0.12616798 0.00105712 0.00134206]\n[0.20460746 0.775072   0.21817294 0.40839905 0.9524199  0.10586923\n 0.156526   0.07449749 0.00783074 0.04750973]\n[0.6085622  0.6839695  0.07832345 0.22699174 0.30072695 0.3384673\n 0.30139816 0.22169363 0.12393287 0.54855514]\n[0.00631496 0.04458407 0.01165989 0.08427769 0.83987916 0.16196379\n 0.30511552 0.01693767 0.42308968 0.00528064]\n[0.09072813 0.8260769  0.02208427 0.09082103 0.98349684 0.02161655\n 0.17927036 0.02793178 0.03087363 0.00147298]\n[0.26275742 0.14861253 0.00483647 0.20674223 0.7056838  0.12812713\n 0.19576362 0.0866873  0.1989651  0.03011855]\n[4.7948897e-02 8.5150623e-01 3.5038516e-01 4.3907166e-03 9.9491405e-01\n 3.7182897e-02 3.3000112e-03 3.6761791e-02 5.9720874e-04 3.1617284e-04]\n[0.00151357 0.79949415 0.52842736 0.06388173 0.78435576 0.2223956\n 0.61093175 0.03422055 0.5147056  0.04846698]\n[0.00450665 0.79052496 0.26083186 0.00566667 0.9551306  0.00158691\n 0.00146577 0.01054356 0.01499224 0.00338787]\n[0.0058808  0.11211669 0.02061433 0.02969766 0.52128094 0.00290111\n 0.18732089 0.05365422 0.08882722 0.01315376]\n[0.00102243 0.9945704  0.58665127 0.09854349 0.9814099  0.01400119\n 0.03350934 0.00880656 0.02509633 0.00902215]\n[0.04938304 0.35152125 0.03284445 0.02505735 0.36530453 0.18375093\n 0.07140043 0.1039682  0.02237943 0.3235609 ]\n[1.00614816e-01 8.46692801e-01 1.00377887e-01 8.08149576e-04\n 9.83704984e-01 2.31187046e-02 8.81314278e-04 3.21586132e-02\n 1.64479017e-04 5.03540039e-04]\n[0.03828174 0.08477876 0.10654241 0.0218502  0.7257613  0.17913496\n 0.19130424 0.0391252  0.11840668 0.00489238]\n[0.01279354 0.2217038  0.03840822 0.00395691 0.31627268 0.09869125\n 0.04977164 0.12805972 0.02762133 0.04497722]\n[0.03649947 0.10434514 0.00611043 0.02125803 0.6356861  0.16842687\n 0.09018487 0.07141271 0.14026555 0.14913824]\n[0.22769484 0.11247534 0.09092566 0.07153955 0.03538042 0.3252172\n 0.20940056 0.25960934 0.04647276 0.22632545]\n[0.07123709 0.163576   0.0022276  0.5772683  0.59118646 0.14450604\n 0.13353631 0.01472023 0.12552175 0.03220287]\n[0.00505447 0.43540654 0.5204564  0.02369064 0.8044821  0.03299993\n 0.01156849 0.00324726 0.07158652 0.01871461]\n[0.08759698 0.11556527 0.05051348 0.22736707 0.6876466  0.22573832\n 0.04205412 0.14452606 0.3839517  0.07078609]\n[0.04524794 0.22872502 0.03306878 0.0219236  0.7313328  0.00952289\n 0.11661506 0.01933217 0.02693936 0.00101146]\n[0.2916674  0.42731145 0.05138877 0.2087161  0.45042714 0.7672301\n 0.19951537 0.17744729 0.05924454 0.4419225 ]\n[3.8476479e-01 6.5765089e-01 2.0611852e-02 1.1103824e-01 9.6975958e-01\n 4.0736645e-02 4.4957876e-02 6.0152858e-02 1.1500283e-04 1.1516511e-03]\n[9.5191598e-03 8.5771441e-01 1.2125710e-01 1.0078788e-02 9.4666719e-01\n 1.1235535e-02 1.2575945e-01 5.4449409e-02 1.1522472e-02 2.3886561e-04]\n[0.7187426  0.1804997  0.0272865  0.405788   0.5239518  0.72750837\n 0.1009858  0.20851898 0.01138076 0.5226908 ]\n[0.0730738  0.21577787 0.00397655 0.7986231  0.32273823 0.11287707\n 0.05734384 0.01311204 0.03024498 0.05023921]\n[0.10224041 0.3053044  0.04980093 0.01694152 0.17209223 0.00932646\n 0.02416843 0.09314623 0.00695246 0.0028944 ]\n[0.07383814 0.635386   0.05135214 0.01321834 0.64959055 0.02023187\n 0.10372934 0.01629207 0.01725435 0.00371718]\n[0.01246697 0.31649157 0.06797388 0.00487873 0.60000545 0.20241445\n 0.0997456  0.07337081 0.02941918 0.00983748]\n[3.7627935e-02 2.9616812e-01 6.4711094e-02 6.8242252e-03 8.2353783e-01\n 1.1127114e-02 5.8117479e-02 1.1686927e-01 1.0975480e-02 3.8337708e-04]\n[0.03157538 0.13307449 0.13431895 0.10590127 0.2764771  0.17407453\n 0.09128261 0.06563953 0.11336097 0.07322925]\n[0.00908482 0.00966051 0.06458533 0.00637987 0.5396825  0.0061295\n 0.00545233 0.0228073  0.124089   0.00609121]\n[0.08462775 0.03822029 0.08188859 0.09364316 0.02631539 0.064051\n 0.04803428 0.03193825 0.04684603 0.15001532]\n[0.4478984  0.23234826 0.20888755 0.8006818  0.58318573 0.28492296\n 0.24802464 0.35523114 0.23075873 0.01549673]\n[0.14548069 0.0842993  0.00974131 0.40002355 0.03632477 0.04806226\n 0.02568871 0.01583758 0.03431427 0.09425509]\n[0.11510816 0.5497413  0.20008391 0.47050425 0.7362571  0.15652469\n 0.20576572 0.07973129 0.10060468 0.12953246]\n[0.01295403 0.16948596 0.03580338 0.06416252 0.7010305  0.32416067\n 0.12500286 0.04242998 0.05839097 0.01603371]\n[0.2860623  0.18887827 0.02195764 0.23806289 0.01778257 0.02055261\n 0.18883976 0.03083611 0.0714123  0.65246636]\n[0.12371016 0.04570171 0.01036894 0.0189254  0.034091   0.05519217\n 0.03333011 0.04160088 0.5815323  0.10939708]\n[0.06520197 0.11501408 0.03464377 0.02954102 0.13225341 0.05498078\n 0.10786107 0.2298297  0.18640444 0.01164883]\n[0.00862414 0.0041894  0.00179935 0.00462824 0.03168344 0.00497594\n 0.04382375 0.02823481 0.21790206 0.00582007]\n[0.04280907 0.00180954 0.00694251 0.00175393 0.19755948 0.00816154\n 0.00199783 0.1138871  0.11121503 0.0023925 ]\n[0.07445034 0.01407558 0.0069176  0.04531616 0.06370404 0.06437889\n 0.0302676  0.04448193 0.46568322 0.03532413]\n[0.02290082 0.00037527 0.0401094  0.00270739 0.11274612 0.00957963\n 0.00840309 0.03840458 0.15909895 0.00065342]\n[0.07865867 0.35801375 0.20649141 0.7259301  0.46405476 0.19543639\n 0.3539021  0.0761044  0.10941535 0.03854021]\n[0.44529667 0.29394674 0.07890511 0.07511315 0.17611545 0.01796517\n 0.06419176 0.05595869 0.00311819 0.00118393]\n[0.01985464 0.00307819 0.0375194  0.28475732 0.20199475 0.11931339\n 0.0171102  0.02854621 0.04130009 0.00636676]\n[0.2908163  0.24814916 0.02619621 0.20936722 0.06078342 0.2738967\n 0.10247082 0.26776737 0.28362533 0.47610265]\n[0.01572537 0.11623916 0.08326274 0.03093976 0.05125213 0.15761131\n 0.4572065  0.4581446  0.3673877  0.04936293]\n[0.00239635 0.61097705 0.13698596 0.00496155 0.6759639  0.00172579\n 0.0066106  0.00719899 0.00622517 0.00390783]\n[0.22326627 0.09926742 0.1836291  0.5340045  0.34216326 0.1732766\n 0.12091252 0.2876884  0.04816249 0.01068997]\n[0.02709213 0.3265451  0.15103689 0.24046707 0.5418878  0.17611828\n 0.2448402  0.0811891  0.26322454 0.0143137 ]\n[0.02471641 0.05620256 0.00606251 0.10944784 0.29728675 0.20718059\n 0.15490946 0.02656993 0.03193632 0.00269124]\n[0.11057705 0.16153118 0.11601433 0.13277104 0.05418822 0.08713311\n 0.06764081 0.06976596 0.02322751 0.02824593]\n[0.01902041 0.02198601 0.02196246 0.03622866 0.02541694 0.03475097\n 0.3649792  0.02277616 0.4344971  0.01323375]\n[0.03866145 0.02516937 0.01036745 0.14051586 0.09442934 0.01180539\n 0.11437336 0.00744879 0.02235213 0.00045437]\n[0.00313744 0.10811135 0.03576183 0.02130789 0.1326009  0.02886885\n 0.03602958 0.00669864 0.1095098  0.00645289]\n[0.1382531  0.16071191 0.03858128 0.31122407 0.09930706 0.01198703\n 0.17130798 0.02810568 0.01948959 0.00194329]\n[0.14869085 0.14861402 0.0778231  0.02962163 0.01179913 0.02193892\n 0.09920242 0.03746143 0.01866001 0.02925131]\n[0.08974454 0.00850397 0.05055013 0.06006956 0.02571094 0.2116763\n 0.01398608 0.11097896 0.01764682 0.02322754]\n[0.04765308 0.05116567 0.04816148 0.0411422  0.00640839 0.02518961\n 0.08915693 0.02714258 0.05505353 0.09140822]\n[0.07373521 0.07035416 0.0967727  0.1103906  0.22838888 0.2438584\n 0.09016696 0.063871   0.10067984 0.02538243]\n[2.7512127e-01 6.3747048e-01 1.9059986e-02 7.3869526e-03 1.2030721e-01\n 4.8692822e-03 3.1028688e-03 5.5036306e-02 5.5083632e-04 2.2971928e-03]\n[0.61758596 0.05228797 0.00942278 0.384337   0.0669474  0.11391708\n 0.00639904 0.08999076 0.06258672 0.3555513 ]\n[0.13929701 0.15363294 0.0132094  0.09112278 0.01668629 0.03431329\n 0.34794813 0.00895283 0.10315937 0.0909555 ]\n[0.06863934 0.09534702 0.04737625 0.41605487 0.10993642 0.02223238\n 0.46066347 0.02593541 0.07052955 0.14767501]\n[0.4882912  0.19739506 0.05942819 0.10003376 0.03066954 0.02173164\n 0.14528531 0.02798247 0.00854117 0.00804865]\n[0.01780716 0.01802766 0.01824942 0.01739338 0.02311084 0.03324455\n 0.23877588 0.02365741 0.41157913 0.01105857]\n[0.02830335 0.01910067 0.00934222 0.00408241 0.02951327 0.0070866\n 0.08893734 0.01442182 0.04406959 0.00088796]\n[3.5063446e-02 8.3149123e-01 1.4551219e-01 4.6449304e-03 3.6254823e-01\n 1.3322145e-02 1.2362897e-03 3.9344311e-02 9.4391209e-05 7.7632070e-04]\n[0.02092254 0.22454587 0.174755   0.03929433 0.05169913 0.14183196\n 0.3141351  0.21145421 0.1632277  0.08870345]\n[0.00551525 0.08766326 0.07447827 0.00548297 0.08808017 0.00161222\n 0.00063685 0.01142514 0.01567692 0.00192326]\n[0.40815997 0.36873046 0.11355829 0.7531624  0.5942697  0.5406566\n 0.05318236 0.3569473  0.07737014 0.2240355 ]\n[0.01501703 0.19774753 0.09638432 0.03748888 0.04015684 0.07210186\n 0.07190993 0.0273591  0.08194655 0.1254462 ]\n[0.09079078 0.11074546 0.11727262 0.02335408 0.00742418 0.03180093\n 0.06020799 0.03811657 0.01600617 0.05222267]\n[0.13456121 0.03022248 0.02116919 0.02588356 0.01331592 0.06243795\n 0.05305544 0.04090872 0.47280267 0.05528304]\n[0.01097172 0.08244154 0.01286176 0.00122023 0.15882647 0.00636712\n 0.01301441 0.08655563 0.02216834 0.000227  ]\n[0.00064987 0.0490211  0.02213901 0.00250292 0.05276188 0.02130604\n 0.00368574 0.00311613 0.0276264  0.03385159]\n[0.13473687 0.01805276 0.01911426 0.5287062  0.16169879 0.09360552\n 0.07245329 0.0295831  0.01292196 0.01122105]\n[0.01604196 0.25394812 0.10663754 0.00499097 0.24629319 0.10504833\n 0.04180771 0.03291404 0.01575255 0.00240877]\n[0.06710231 0.11376628 0.09746656 0.24838242 0.2179949  0.07017305\n 0.24437037 0.08309633 0.1352019  0.00734475]\n[0.00627762 0.00405106 0.00736925 0.05263278 0.02279922 0.04033235\n 0.15683067 0.02564609 0.06680009 0.00782058]\n[0.14537269 0.08174226 0.00483    0.09391165 0.00988522 0.0190154\n 0.04026788 0.01955551 0.08030471 0.46641776]\n[5.80710173e-03 7.83309370e-05 8.87399912e-03 1.27233565e-02\n 1.16856545e-01 9.26163793e-03 4.25097346e-03 1.16392672e-02\n 2.07974762e-01 1.30641460e-03]\n[0.0041258  0.21100473 0.10749424 0.18070078 0.34421617 0.1316449\n 0.20233142 0.0428994  0.18592554 0.00939402]\n[0.05170089 0.16406927 0.07441577 0.29259008 0.24107909 0.38490602\n 0.1747534  0.25243455 0.6606517  0.07353985]\n[0.00647458 0.32961088 0.08122268 0.00275961 0.10433775 0.00279063\n 0.00175753 0.01974556 0.00753957 0.00096518]\n[0.00942111 0.06486785 0.06409723 0.01144838 0.35831612 0.00232783\n 0.00071523 0.01433301 0.03157219 0.00099117]\n[0.24027923 0.0595375  0.06416908 0.01024026 0.00430787 0.02279893\n 0.10550949 0.05752662 0.01061317 0.01527485]\n[0.43862295 0.2534285  0.14132619 0.26194227 0.1825127  0.11342686\n 0.0703339  0.12852255 0.09078673 0.03056294]\n[0.22006202 0.00764731 0.05513132 0.45680726 0.10212168 0.26077402\n 0.03882769 0.050006   0.2677287  0.00699115]\n[0.00710061 0.03755823 0.03267503 0.11013609 0.16603374 0.12027749\n 0.0543941  0.02714235 0.3867584  0.00926033]\n[0.00638539 0.02424616 0.02624086 0.00550029 0.2046138  0.01388511\n 0.03636155 0.04881123 0.12854472 0.00052562]\n[4.3082237e-04 8.9668059e-01 5.4113775e-01 3.0131161e-02 3.2531261e-01\n 1.3742149e-03 1.2337357e-02 4.0226281e-03 5.0014526e-02 2.6946962e-02]\n[0.08213702 0.43350452 0.01739788 0.00465196 0.02737758 0.02413064\n 0.00607342 0.0536525  0.0016017  0.0004735 ]\n[0.03148985 0.05175447 0.11940742 0.3060062  0.13294297 0.09724608\n 0.07595652 0.05939028 0.34949052 0.02200326]\n[0.00416753 0.6467694  0.11605915 0.01532319 0.09634933 0.01044762\n 0.09568223 0.01716682 0.00988725 0.00100192]\n[2.86810398e-02 8.40321064e-01 1.23274624e-01 4.54208255e-03\n 2.47411549e-01 1.20407343e-02 1.23864412e-03 3.25560272e-02\n 9.68127279e-05 6.35504723e-04]\n[0.5097677  0.04275629 0.02652371 0.39954233 0.12039363 0.1720866\n 0.01037756 0.0531618  0.06871501 0.23751754]\n[0.13943657 0.00668997 0.01159814 0.53085786 0.05576447 0.07888672\n 0.03117278 0.04709551 0.00948998 0.00531074]\n[0.51186967 0.10315463 0.01716915 0.25404954 0.03562626 0.21864158\n 0.05037296 0.18746445 0.29159033 0.4680735 ]\n[0.08943042 0.03048351 0.02163929 0.03358194 0.02435306 0.0348269\n 0.06428471 0.01999921 0.3951817  0.08823833]\n[0.07360795 0.03036693 0.00798771 0.04594707 0.07339349 0.02932876\n 0.03980035 0.13692415 0.02015427 0.00115678]\n[0.0011397  0.5261006  0.40337428 0.01787999 0.08986077 0.02166027\n 0.22925642 0.00443983 0.11209148 0.01459974]\n[6.6158384e-02 1.8124190e-01 5.6485981e-02 1.9370526e-02 5.5824542e-01\n 1.7485887e-02 4.5087546e-02 1.2405279e-01 7.4093640e-03 2.3287535e-04]\n[0.0762181  0.00393093 0.05790201 0.04562312 0.01298019 0.04324716\n 0.01552591 0.03026408 0.04676121 0.0181599 ]\n[3.05048823e-02 8.18393350e-01 1.12151414e-01 4.30813432e-03\n 1.84383601e-01 1.09031200e-02 9.67741013e-04 3.54351699e-02\n 8.82102977e-05 5.67436218e-04]\n[0.01803228 0.01163748 0.01311043 0.1459297  0.04901919 0.12240607\n 0.02918223 0.03457886 0.14052531 0.00981638]\n[0.00861901 0.01281041 0.05134764 0.00731981 0.03171822 0.03019014\n 0.2328679  0.09095177 0.4505439  0.01234332]\n[0.00079626 0.75021935 0.34083903 0.04132774 0.18492395 0.00623396\n 0.09926346 0.00969321 0.11275992 0.00703156]\n[5.0944567e-02 7.1551186e-01 5.9203506e-02 7.8910291e-03 1.0961196e-01\n 9.0825260e-03 2.3451149e-03 1.8388599e-02 2.0402670e-04 2.3215711e-03]\n[0.00818774 0.01280865 0.01674244 0.00338456 0.12060764 0.00709155\n 0.00240204 0.01700523 0.07466006 0.00673774]\n[0.08663565 0.06442836 0.03914091 0.11503306 0.0355916  0.02567652\n 0.22367978 0.01569182 0.16704786 0.04203501]\n[0.7191793  0.44504714 0.07776809 0.29365408 0.08849978 0.28544492\n 0.21166137 0.1938242  0.02785182 0.6511238 ]\n[0.8106859  0.5110178  0.09953871 0.42698646 0.10682008 0.3679814\n 0.20500404 0.32133842 0.01299113 0.7154114 ]\n[0.02999741 0.01036027 0.01846531 0.03786016 0.08825311 0.02720466\n 0.00667965 0.00424674 0.07969636 0.00321504]\n[0.00123128 0.6790948  0.43674177 0.00958213 0.47341737 0.00570118\n 0.02045903 0.00377277 0.02881858 0.01076412]\n[0.00330669 0.00333315 0.0063594  0.04435888 0.02651888 0.04022357\n 0.0324918  0.01600727 0.32039768 0.04350296]\n[0.01291671 0.06506482 0.09669781 0.03995016 0.25624472 0.02982688\n 0.00408062 0.03517941 0.07485041 0.00125173]\n[0.14721382 0.00528038 0.01352268 0.02142477 0.00325224 0.0118407\n 0.05297977 0.02496839 0.04878092 0.01200494]\n[3.07688415e-02 8.19615901e-01 1.05112076e-01 5.45218587e-03\n 1.42076522e-01 1.74377859e-02 2.92640924e-03 2.34353542e-02\n 1.47521496e-04 1.09234452e-03]\n[0.0325782  0.0188587  0.0084174  0.0416835  0.01307321 0.01566795\n 0.20338264 0.01732436 0.1145404  0.00502637]\n[0.00385165 0.0036709  0.0033389  0.02074447 0.01739621 0.01289037\n 0.03151798 0.01030946 0.24078563 0.01688749]\n[2.9599100e-02 2.4005929e-01 8.1737578e-02 1.9064221e-01 5.8395779e-01\n 1.5997043e-01 1.4034724e-01 1.9629091e-02 2.8997660e-03 3.8212538e-04]\n[3.5485625e-04 4.9440837e-01 4.5971096e-02 9.9503696e-03 4.8936695e-02\n 5.8360696e-03 1.6836256e-02 7.3930621e-03 7.5222641e-02 5.1851273e-03]\n[0.00191808 0.6357013  0.3920552  0.01900432 0.42114782 0.009619\n 0.03965953 0.01113731 0.08126038 0.00603873]\n[0.01531816 0.15981925 0.06325105 0.13897121 0.16507217 0.04815072\n 0.33213425 0.02739093 0.0462698  0.00114781]\n[0.3314522  0.12364051 0.24193871 0.83690375 0.21088937 0.24224678\n 0.14381585 0.2909273  0.1414383  0.01178357]\n[0.18900785 0.01711422 0.11499223 0.46001327 0.06235152 0.08165053\n 0.08157396 0.17712986 0.07523242 0.00709885]\n[0.5072842  0.04695475 0.01223281 0.12971297 0.00538141 0.03711092\n 0.06321049 0.02626485 0.03693864 0.55802363]\n[0.00147149 0.0075849  0.08846217 0.04138142 0.07464486 0.10144025\n 0.08351228 0.0056968  0.17590138 0.00364241]\n[0.09158435 0.00679427 0.0326694  0.35950014 0.09500673 0.10506785\n 0.0155873  0.03894883 0.4063262  0.00543621]\n[0.11092591 0.01458845 0.1399439  0.07295761 0.01250994 0.04738259\n 0.03108147 0.02503666 0.02192959 0.03237838]\n[0.13396731 0.0074169  0.09345129 0.09109062 0.01928407 0.11163706\n 0.01530507 0.04796058 0.02588782 0.02442497]\n[0.01070374 0.2959621  0.1840752  0.34526175 0.40925652 0.20483705\n 0.04763022 0.02956066 0.1269027  0.02325618]\n[0.00316983 0.00670755 0.1038585  0.00379929 0.00464231 0.0048832\n 0.00444889 0.01168078 0.01604784 0.00262094]\n[1.48177445e-02 6.34171009e-01 1.79583758e-01 3.87534499e-03\n 1.65516943e-01 1.93261802e-02 2.65401602e-03 1.78841054e-02\n 3.67790461e-04 1.36166811e-04]\n[0.00126755 0.02180627 0.09655342 0.0384872  0.0581381  0.0255481\n 0.15063825 0.0041526  0.16274118 0.00691608]\n[0.00916627 0.21987423 0.31950766 0.02618501 0.19927374 0.02061456\n 0.06229404 0.01554793 0.21965623 0.00245565]\n[0.00612617 0.05222517 0.06606057 0.02190441 0.26291147 0.04135159\n 0.0123831  0.04594776 0.05973232 0.00044587]\n[0.17646024 0.06031504 0.12079379 0.05158931 0.00497416 0.11348608\n 0.02861759 0.11689082 0.00401244 0.1089876 ]\n[2.1339655e-02 7.7525002e-01 1.3600248e-01 7.4254274e-03 1.5657592e-01\n 1.6044736e-02 2.0508766e-03 2.9729337e-02 2.1004677e-04 4.2337179e-04]\n[0.01004949 0.11295414 0.05292174 0.00503188 0.04837668 0.02268237\n 0.17985475 0.03190339 0.07969818 0.05876026]\n[0.02849519 0.02455631 0.00768387 0.05528003 0.02253628 0.00489905\n 0.05445668 0.01402342 0.01333126 0.00028768]\n[0.00433409 0.00046295 0.01944742 0.04562536 0.01234642 0.00449497\n 0.01423663 0.00807533 0.20238563 0.00240448]\n[2.02572346e-02 8.69549155e-01 1.01705551e-01 4.79128957e-03\n 1.63697511e-01 1.09089315e-02 1.13484263e-03 2.53567100e-02\n 1.17200623e-04 4.10795212e-04]\n[0.1473243  0.02177277 0.15838823 0.08486518 0.01086569 0.12363121\n 0.07315066 0.15686858 0.01358294 0.05128831]\n[0.00171655 0.01862308 0.01699698 0.149434   0.00284493 0.00162673\n 0.05328903 0.01024002 0.11656916 0.02527431]\n[1.9787729e-02 7.8426433e-01 1.2366167e-01 6.2948465e-03 1.0737091e-01\n 1.5565723e-02 2.2808611e-03 1.4222473e-02 2.1299720e-04 1.2131929e-03]\n[0.11684689 0.00757161 0.03900987 0.5904981  0.18457887 0.21706372\n 0.05084068 0.07730168 0.20861688 0.00367072]\n[0.03396115 0.11241522 0.17205414 0.01984009 0.03078023 0.09648818\n 0.27994967 0.08095369 0.11900064 0.11068609]\n[0.00775537 0.01544571 0.12481579 0.05001757 0.00138417 0.01856571\n 0.16155258 0.04585415 0.17955494 0.00582141]\n[0.03208023 0.02319226 0.17515138 0.43985084 0.07588726 0.1462236\n 0.09222978 0.19429925 0.21618327 0.00273213]\n[0.00977525 0.07125396 0.00889784 0.01089975 0.07114697 0.06272787\n 0.08765009 0.03762597 0.13688949 0.00081533]\n[0.00283244 0.03797624 0.05214688 0.00917295 0.12726206 0.02634957\n 0.01782727 0.04393083 0.08336553 0.00143075]\n[0.00249216 0.4718465  0.44901553 0.00461096 0.4980515  0.00728655\n 0.02019385 0.00329205 0.01972657 0.00901452]\n[0.6093294  0.5814326  0.08127278 0.31072322 0.09630552 0.18421096\n 0.45146725 0.15921709 0.04726797 0.57281303]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3f24f8a6bf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRBM_hidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_rbms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRBM_hidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrbm_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d95e9a6bf963>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m                         range(0, len(self._X), self._batchsize), range(self._batchsize, len(self._X), self._batchsize)):\n\u001b[1;32m     80\u001b[0m                     \u001b[0;31m# Run the training operation on the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     _,t=sess.run([train_op,_a[-1]], feed_dict={\n\u001b[0m\u001b[1;32m     82\u001b[0m                         _a[0]: self._X[start:end], y: self._Y[start:end]})\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2py3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2py3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2py3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1359\u001b[0m                            run_metadata)\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2py3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2py3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2py3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1440\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1441\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    nNet = NN(RBM_hidden_sizes, trX, trY)\n",
    "    nNet.load_from_rbms(RBM_hidden_sizes,rbm_list)\n",
    "    nNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}