{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tk5ROC6mw0f"
   },
   "source": [
    "# Text Summarization of Amazon reviews\n",
    "\n",
    "In this notebook I will write summaries with the help of my Seq2Seq model in Summarizer.py.\n",
    "\n",
    "The model works impressively well in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nRequirement already satisfied: nltk in /root/anaconda3/envs/t/lib/python3.6/site-packages (3.5)\nRequirement already satisfied: regex in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (2020.7.14)\nRequirement already satisfied: joblib in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (0.16.0)\nRequirement already satisfied: click in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: tqdm in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (4.48.2)\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B_ULQF2smw0h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from collections import Counter\n",
    "\n",
    "import Summarizer\n",
    "import summarizer_data_utils\n",
    "import summarizer_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.14.0\n"
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XBIbvs4mw0m"
   },
   "source": [
    "## The data\n",
    "\n",
    "\n",
    "The data we will be using with is a dataset from Kaggle, the Amazon Fine Food Reviews dataset.  \n",
    "It contains, as the name suggests, 570.000 reviews of fine foods from Amazon and summaries of those reviews. \n",
    "Our aim is to input a review (Text column) and automatically create a summary (Summary colum) for it.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NYVncFKmw0n"
   },
   "source": [
    "### Reading and exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4894,
     "status": "ok",
     "timestamp": 1526227108183,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "2OiSxpApmw0o",
    "outputId": "98a255ad-248e-4f1e-b43f-1c8d384fd453"
   },
   "outputs": [],
   "source": [
    "# load csv file using pandas.\n",
    "#file_path = './Reviews.csv'\n",
    "#data = pd.read_csv(file_path)\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rElWMbT2mw0t",
    "outputId": "08f4b9ee-8c78-4f3b-c986-f7f3dd2ff248"
   },
   "outputs": [],
   "source": [
    "# we will only use the last two columns Summary (target) and Text (input).\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1526227113630,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "N9bHztjpmw0x",
    "outputId": "aa4ce93d-efde-4ebb-bc3e-7b03d477a5e4"
   },
   "outputs": [],
   "source": [
    "# check for missings --> got some in summary drop those. \n",
    "# 26 are missing, so we will drop those!\n",
    "#data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "boMCgsgTmw00"
   },
   "outputs": [],
   "source": [
    "# drop row, if values in Summary is missing. \n",
    "#data.dropna(subset=['Summary'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1526227125421,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ESv4XLgQmw03",
    "outputId": "0ca5d3f7-7efc-46dc-bf8e-bb5803c6376c"
   },
   "outputs": [],
   "source": [
    "# only summary and text are useful for us.\n",
    "#data = data[['Summary', 'Text']]\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BjmIGbXtmw08"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nraw_texts = []\\nraw_summaries = []\\n\\nfor text, summary in zip(data.Text, data.Summary):\\n    if 100< len(text) < 150:\\n        raw_texts.append(text)\\n        raw_summaries.append(summary)\\n        '"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# we will not use all of them, only short ones and ones of similar size. \n",
    "# choosing the ones that are of similar length makes it easier for the model to learn.\n",
    "'''\n",
    "raw_texts = []\n",
    "raw_summaries = []\n",
    "\n",
    "for text, summary in zip(data.Text, data.Summary):\n",
    "    if 100< len(text) < 150:\n",
    "        raw_texts.append(text)\n",
    "        raw_summaries.append(summary)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1560,
     "status": "ok",
     "timestamp": 1526227148045,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "t5JBoyqKmw0_",
    "outputId": "150af52c-a0af-4eec-f0ff-399ec33e434d"
   },
   "outputs": [],
   "source": [
    "#len(raw_texts), len(raw_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tMsDeec4mw1F",
    "outputId": "de46147e-d2c4-40e5-9a0a-22f27ac360fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for t, s in zip(raw_texts[:5], raw_summaries[:5]):\n",
    " #   print('Text:\\n', t)\n",
    "  #  print('Summary:\\n', s, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-CyKX1gmw1J"
   },
   "source": [
    "### Clean and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47800,
     "status": "ok",
     "timestamp": 1526227313932,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "4CLoTqyzmw1K",
    "outputId": "2fcb8327-e714-48e2-fca1-6bf39b8e6411",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nimport nltk\\nnltk.download('punkt')\\nprocessed_texts, processed_summaries, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\\n    raw_texts,\\n    raw_summaries,\\n    keep_most=False\\n)\\nprint(processed_texts[:3])\\nprint(processed_summaries[:3])\\nprint(len(words_counted))\\nprint(words_counted[:5])\\n\""
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# the function gives us the option to keep_most of the characters inisde the texts and summaries, meaning\n",
    "# punctuation, question marks, slashes...\n",
    "# or we can set it to False, meaning we only want to keep letters and numbers like here.\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "processed_texts, processed_summaries, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\n",
    "    raw_texts,\n",
    "    raw_summaries,\n",
    "    keep_most=False\n",
    ")\n",
    "print(processed_texts[:3])\n",
    "print(processed_summaries[:3])\n",
    "print(len(words_counted))\n",
    "print(words_counted[:5])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yquphHYJmw1R",
    "outputId": "cd9ad917-33da-4294-eb41-f3b0abb967e9",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.702 seconds.\nPrefix dict has been built successfully.\n[('的', 4755178), ('０', 3961908), ('\\u3000', 3895280)]\n[['南', '都', '讯', '\\u3000', '记者', '刘凡', '\\u3000', '周昌', '和', '\\u3000', '任笑', '一', '\\u3000', '继', '推出', '日票', '后', '深圳', '今后', '将', '设', '地铁', 'Ｖ', 'Ｉ', 'Ｐ', '头等', '车厢', '设', '坐票', '制', '昨日', '《', '南', '都', 'Ｍ', 'Ｅ', 'Ｔ', 'Ｒ', 'Ｏ', '》', '创刊', '仪式', '暨', '２', '０', '１', '２', '年', '深港', '地铁', '圈', '高峰论坛', '上', '透露', '在', '未来', '的', '１', '１', '号线', '上将', '增加', '特色', '服务', '满足', '不同', '消费', '层次', '的', '乘客', '的', '不同', '需求', '如', '特设', '行李架', '的', '车厢', '和', '买', '双倍', '票', '可', '有', '座位', '坐', '的', 'Ｖ', 'Ｉ', 'Ｐ', '车厢', '等', '\\ue40c', '论坛', '上', '深圳市政府', '副', '秘书长', '轨道交通', '建设', '办公室', '主任', '赵鹏林', '透露', '地铁', '未来', '的', '方向', '将', '分等级', '满足', '不同', '层次', '的', '人', '的', '需求', '提供', '不同', '层次', '的', '有', '针对', '的', '服务', '其中', '包括', '一些', '档次', '稍微', '高', '一些', '的', '服务'], ['同心县', '地处', '宁夏', '中部', '干旱', '带', '的', '核心区', '\\u3000', '冬寒', '长', '春暖迟', '夏热', '短', '秋凉', '早', '干旱', '少雨', '蒸发', '强烈', '风大沙', '多', '主要', '自然灾害', '有', '沙尘暴', '干热风', '霜冻', '冰雹', '等', '其中', '以', '干旱', '危害', '最为', '严重', '\\ue40c', '由于', '生态环境', '的', '极度', '恶劣', '导致', '农村', '经济', '发展缓慢', '人民', '群众', '生产', '生活', '水平', '低下', '靠天吃饭', '的', '被动局面', '依然', '存在', '同心', '又', '是', '国家级', '老', '少', '边', '穷县', '之一', '…', '［', '详细', '］'], ['不满', '一岁', '的', '永康', '是', '个', '饱经', '病痛', '折磨', '的', '孩子', '２', '０', '１', '１', '年', '７', '月', '５', '日', '出生', '的', '他', '患有', '先天性', '心脏病', '疝气', '一', '出生', '便', '被遗弃', '２', '０', '１', '２', '年', '１', '月', '８', '日', '才', '５', '个', '月', '大', '的', '永康', '被', '发现', '呼吸困难', '随后', '送往', '医院', '进行', '抢救', '治疗', '病情', '稳定', '后于', '１', '月', '２', '８', '日', '出院', '\\ue40c', '２', '０', '１', '２', '年', '２', '月', '１', '３', '号', '永康', '在', '思源', '焦点', '公益', '基金', '的', '帮助', '下', '在', '医院', '接受', '手术', '治疗', '术后', '仅', '８', '天', '永康', '突发', '右侧', '腹股沟', '斜', '疝', '嵌顿', '及', '肠梗阻', '又', '再次', '进行', '抢救', '治疗', '术后', '进', '重症', '监护室', '３', '月', '７', '日', '几经', '病痛', '折磨', '的', '永康', '终于', '康复', '出院', '目前', '他', '的', '病情', '已经', '稳定']]\n[['深圳', '地铁', '将', '设立', 'Ｖ', 'Ｉ', 'Ｐ', '头等', '车厢', '\\u3000', '买', '双倍', '票', '可享', '坐票'], ['中国', '西部', '是', '地球', '上', '主要', '干旱', '带', '之一', '妇女', '是', '当地', '劳动力', '．', '．', '．'], ['思源', '焦点', '公益', '基金', '救助', '孩子', '永康']]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nprocessed_content, processed_title, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\\n    content,\\n    title,\\n    keep_most=False\\n)'"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#for t,s in zip(processed_texts[:5], processed_summaries[:5]):\n",
    " #   print('Text\\n:', t, '\\n')\n",
    "  #  print('Summary:\\n', s, '\\n\\n\\n')\n",
    "\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow.keras as keras\n",
    "max_features=10000\n",
    "maxlen=300\n",
    "dt=pd.read_csv(\"sgnewfull.csv\",sep='\\t',skiprows=1,names=['title','content'])\n",
    "#按行读取文件，返回文件的行字符串列表,读取stopwords.dat\n",
    "def read_file(file_name):\n",
    "    fp = open(file_name, \"r\", encoding=\"utf-8\")\n",
    "    content_lines = fp.readlines()\n",
    "    fp.close()\n",
    "    #去除行末的换行符，否则会在停用词匹配的过程中产生干扰\n",
    "    for i in range(len(content_lines)):\n",
    "        content_lines[i] = content_lines[i].rstrip(\"\\n\")\n",
    "    return content_lines\n",
    "def fenci(selist):\n",
    "    stopwords = read_file(\"stopwords.dat\")#读取停用词\n",
    "    l=[]\n",
    "    for i in selist:\n",
    "        k=[]\n",
    "        seg_list = jieba.cut(i)  # 默认是精确模式\n",
    "        outstr=''\n",
    "        i=0\n",
    "        for word in seg_list:  #去除停顿词\n",
    "            i+=1\n",
    "            if word not in stopwords:  #如果去除停用词的话，把注释去掉，同时把下面三行加tab\n",
    "                if word != '\\t' and i<150 :  \n",
    "                    k.append(word)\n",
    "        l.append(k)\n",
    "    return l\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features,lower=True)\n",
    "dt['title']=dt['title'].fillna(\"\")\n",
    "dt['content']=dt['content'].fillna(\"\")\n",
    "title=dt['title']\n",
    "#title=[('s'+i+'e')for i in title]\n",
    "\n",
    "content=fenci(list(dt['content']))\n",
    "title=fenci(title)\n",
    "#统计词\n",
    "words=[]\n",
    "for text in content:\n",
    "    for word in text:\n",
    "        words.append(word)\n",
    "for text in title:\n",
    "    for word in text:\n",
    "        words.append(word)\n",
    "words_counted = Counter(words).most_common() \n",
    "print(words_counted[:3])\n",
    "print(content[:3])     \n",
    "print(title[:3])\n",
    "'''\n",
    "processed_content, processed_title, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\n",
    "    content,\n",
    "    title,\n",
    "    keep_most=False\n",
    ")'''\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer.fit_on_texts(content + \n",
    " #                      title)\n",
    "#content=tokenizer.texts_to_sequences(content)\n",
    "#content=keras.preprocessing.sequence.pad_sequences(content,maxlen=maxlen) # padding\n",
    "#title=tokenizer.texts_to_sequences(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34eUnqVQmw1c"
   },
   "source": [
    "### Create lookup dicts\n",
    "\n",
    "We cannot feed our network actual words, but numbers. So we first have to create our lookup dicts, where each words gets and int value (high or low, depending on its frequency in our corpus). Those help us to later convert the texts into numbers.\n",
    "\n",
    "We also add special tokens. EndOfSentence and StartOfSentence are crucial for the Seq2Seq model we later use.\n",
    "Pad token, because all summaries and texts in a batch need to have the same length, pad token helps us do that.\n",
    "\n",
    "So we need 2 lookup dicts:\n",
    " - From word to index \n",
    " - from index to word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1526227336251,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "zwqbTP8jmw1d",
    "outputId": "3788ccc5-bec0-4d32-8885-0698b45d7164",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "662369 662369 0\n"
    }
   ],
   "source": [
    "specials = [\"<EOS>\", \"<SOS>\",\"<PAD>\",\"<UNK>\"]\n",
    "word2ind, ind2word,  missing_words = summarizer_data_utils.create_word_inds_dicts(words_counted,\n",
    "                                                                       specials = specials)\n",
    "print(len(word2ind), len(ind2word), len(missing_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVZ1Qmk9mw1j"
   },
   "source": [
    "### Pretrained embeddings\n",
    "\n",
    "Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy.\n",
    "Here I used two different options. Either we use glove embeddings or embeddings from tf_hub.\n",
    "The ones from tf_hub worked better, so we use those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embeddings_path = './glove.6B.300d.txt'\n",
    "# embedding_matrix_save_path = './embeddings/my_embedding_github.npy'\n",
    "# emb = summarizer_data_utils.create_and_save_embedding_matrix(word2ind,\n",
    "#                                                              glove_embeddings_path,\n",
    "#                                                              embedding_matrix_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61662,
     "status": "ok",
     "timestamp": 1526227413054,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ObE6ggfAmw1o",
    "outputId": "e12b1f22-0fad-4a3d-e934-5fc480b83788"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nembed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\\nemb = embed([key for key in word2ind.keys()])\\n\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    sess.run(tf.tables_initializer())\\n    embedding = sess.run(emb)\\n'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# the embeddings from tf_hub. \n",
    "# embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "'''\n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\n",
    "emb = embed([key for key in word2ind.keys()])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding = sess.run(emb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1526227413774,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ayXi9D7Umw1u",
    "outputId": "7b5b5522-8c21-4c70-a5be-46f6ed2cdbd2"
   },
   "outputs": [],
   "source": [
    "#embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QoGa9EWdmw11"
   },
   "outputs": [],
   "source": [
    "#np.save('./tf_hub_embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QV1HB3zzmw12"
   },
   "source": [
    "### Convert text and summaries\n",
    "\n",
    "As I said before we cannot feed the words directly to our network, we have to convert them to numbers first of all. This is what we do here. And we also append the SOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NjudfxFPmw13"
   },
   "outputs": [],
   "source": [
    "# converts words in texts and summaries to indices\n",
    "# it looks like we have to set eos here to False\n",
    "processed_texts=content\n",
    "converted_texts, unknown_words_in_texts = summarizer_data_utils.convert_to_inds(processed_texts,\n",
    "                                                                                word2ind,\n",
    "                                                                                eos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1dFsLoAqmw16"
   },
   "outputs": [],
   "source": [
    "processed_summaries=title\n",
    "converted_summaries, unknown_words_in_summaries = summarizer_data_utils.convert_to_inds(processed_summaries,word2ind, eos = True,  sos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2143,
     "status": "ok",
     "timestamp": 1526227545460,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ghATcyE4mw2A",
    "outputId": "06d3f934-7c97-49e2-c358-21bd7552689d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(converted_texts[:3])\n",
    "#print(converted_summaries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1526227550694,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "pSTzMURHmw2E",
    "outputId": "4dc8b405-f811-49f2-b7b3-83495dbf7640",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seems to have worked well. \n",
    "#print( summarizer_data_utils.convert_inds_to_text(converted_texts[0], ind2word),\n",
    "     #  summarizer_data_utils.convert_inds_to_text(converted_summaries[0], ind2word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8b9Nd0zmw2H"
   },
   "source": [
    "## The model\n",
    "\n",
    "Now we can build and train our model. First we define the hyperparameters we want to use. Then we create our Summarizer and call the function .build_graph(), which as the name suggests, builds the computation graph. \n",
    "Then we can train the model using .train()\n",
    "\n",
    "After training we can try our model using .infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2z9xOKzmw2I"
   },
   "source": [
    "### Training\n",
    "\n",
    "We can optionally use a cyclic learning rate, which we do here. \n",
    "I trained the model for 20 epochs and the loss was low then, but we could train it longer and would probably get better results.\n",
    "\n",
    "Unfortunately I do not have the resources to find the perfect (or right) hyperparameters, but these do pretty well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tEItjpP4mw2J"
   },
   "outputs": [],
   "source": [
    "# model hyperparametes\n",
    "num_layers_encoder = 2\n",
    "num_layers_decoder = 2\n",
    "rnn_size_encoder = 128\n",
    "rnn_size_decoder = 128\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "clip = 5\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.001\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 700\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = './tf_hub_embedding.npy'\n",
    "summary_dir = os.path.join('./tensorboard', str('Nn_' + str(rnn_size_encoder) + '_Lr_' + str(learning_rate)))\n",
    "\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1464,
     "status": "ok",
     "timestamp": 1526234914336,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "u8lJ_OI5mw2Q",
    "outputId": "1c06bc51-01eb-4a68-b4ca-38d56a4a2a76"
   },
   "outputs": [],
   "source": [
    "#len(converted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1526234915582,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "w_VDuiHyQK84",
    "outputId": "9bc17a2e-837b-41bd-a40d-0f116e143d8f"
   },
   "outputs": [],
   "source": [
    "d=round(len(converted_summaries)*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from numba import cuda\n",
    "#cuda.select_device(0)\n",
    "#cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85881
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8531236,
     "status": "error",
     "timestamp": 1526243447242,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "E0BX6Z7Kmw2T",
    "outputId": "e734411e-2fbc-4960-e2aa-fb98108c4576",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /root/my/summarizer_model_utils.py:117: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n\nWARNING:tensorflow:From /root/my/summarizer_model_utils.py:118: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:108: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:126: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /root/my/Summarizer.py:615: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:157: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From /root/my/Summarizer.py:158: The name tf.nn.rnn_cell.DropoutWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.DropoutWrapper instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:254: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/my/Summarizer.py:259: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:383: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py:107: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.random.categorical` instead.\nWARNING:tensorflow:From /root/my/Summarizer.py:221: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:104: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\nGraph built.\n-------------------- Epoch 0 of 10 --------------------\n"
    },
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dynamic_seq2seq/Adam/update/_182]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3':\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/../pyvsc-run-isolated.py\", line 24, in <module>\n    runpy.run_path(module, run_name=\"__main__\")\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/kernel_prewarm_starter.py\", line 31, in <module>\n    runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 208, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-27-cb70788c0a65>\", line 23, in <module>\n    summarizer.build_graph()\n  File \"/root/my/Summarizer.py\", line 103, in build_graph\n    self.add_seq2seq()\n  File \"/root/my/Summarizer.py\", line 197, in add_seq2seq\n    encoder_state)\n  File \"/root/my/Summarizer.py\", line 302, in build_decoder\n    scope=decoder_scope\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in dynamic_decode\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in <lambda>\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1205, in stack\n    return self._implementation.stack(name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 309, in stack\n    return self.gather(math_ops.range(0, self.size()), name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 323, in gather\n    element_shape=element_shape)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6705, in tensor_array_gather_v3\n    element_shape=element_shape, name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dynamic_seq2seq/Adam/update/_182]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb70788c0a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                  \u001b[0mconverted_summaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                  \u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverted_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                  validation_targets=converted_summaries[d:])\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my/Summarizer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, targets, restore_path, validation_inputs, validation_targets)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;31m# run training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my/Summarizer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, inputs, targets, epoch)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     _, train_loss = self.sess.run([self.train_op, self.train_loss],\n\u001b[0;32m--> 523\u001b[0;31m                                                   feed_dict=fd)\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnbatches\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dynamic_seq2seq/Adam/update/_182]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3':\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/../pyvsc-run-isolated.py\", line 24, in <module>\n    runpy.run_path(module, run_name=\"__main__\")\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/kernel_prewarm_starter.py\", line 31, in <module>\n    runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 208, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-27-cb70788c0a65>\", line 23, in <module>\n    summarizer.build_graph()\n  File \"/root/my/Summarizer.py\", line 103, in build_graph\n    self.add_seq2seq()\n  File \"/root/my/Summarizer.py\", line 197, in add_seq2seq\n    encoder_state)\n  File \"/root/my/Summarizer.py\", line 302, in build_decoder\n    scope=decoder_scope\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in dynamic_decode\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in <lambda>\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1205, in stack\n    return self._implementation.stack(name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 309, in stack\n    return self.gather(math_ops.range(0, self.size()), name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 323, in gather\n    element_shape=element_shape)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6705, in tensor_array_gather_v3\n    element_shape=element_shape, name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# build graph and train the model \n",
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   save_path='./models/sogou/my_model',\n",
    "                                   mode='TRAIN',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   batch_size = 32,\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = keep_probability,\n",
    "                                   learning_rate = learning_rate,\n",
    "                                   max_lr=max_lr,\n",
    "                                   learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                                   learning_rate_decay = learning_rate_decay,\n",
    "                                   epochs = epochs,\n",
    "                                   pretrained_embeddings_path = None, #pretrained_embeddings_path,\n",
    "                                   use_cyclic_lr = use_cyclic_lr,\n",
    "                                   summary_dir = None)#summary_dir)           \n",
    "\n",
    "summarizer.build_graph()\n",
    "summarizer.train(converted_texts[:d], \n",
    "                 converted_summaries[:d],\n",
    "                 validation_inputs=converted_texts[d:],\n",
    "                 validation_targets=converted_summaries[d:])\n",
    "\n",
    "\n",
    "# hidden training output.\n",
    "# both train and validation loss decrease nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5Hqzvocmw2W"
   },
   "source": [
    "### Inference\n",
    "Now we can use our trained model to create summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4607,
     "status": "ok",
     "timestamp": 1526243454761,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ljN9a1hemw2Y",
    "outputId": "f60102af-44f0-4c45-8ba3-2548e5af0a4c",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939879efd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f939869c438>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f938063f080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f938063f080>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f938063f080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f938063f080>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f938063f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f938063f160>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f938063f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f938063f160>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f93986892e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f93986892e8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f93986892e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f93986892e8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9398270da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9398270da0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9398270da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9398270da0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f938071df28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f938071df28>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f938071df28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f938071df28>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9380693940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9380693940>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9380693940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9380693940>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f93980972b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f93980972b0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f93980972b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f93980972b0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:985: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nGraph built.\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\n"
    },
    {
     "output_type": "error",
     "ename": "NotFoundError",
     "evalue": "./models/sogou; No such file or directory",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d1a9b234d0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m preds = summarizer.infer(converted_texts[:50],\n\u001b[1;32m     20\u001b[0m                          \u001b[0mrestore_path\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m'./models/sogou/my_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                          targets = converted_summaries[:50])\n\u001b[0m",
      "\u001b[0;32m~/my/Summarizer.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, inputs, restore_path, targets)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mprediction_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my/Summarizer.py\u001b[0m in \u001b[0;36mrestore_session\u001b[0;34m(self, restore_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[1;32m   1278\u001b[0m                        compat.as_text(save_path))\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m    370\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m    371\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m--> 372\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 384\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    385\u001b[0m     ]\n\u001b[1;32m    386\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./models/sogou; No such file or directory"
     ]
    }
   ],
   "source": [
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   './models/sogou/my_model',\n",
    "                                   'INFER',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   batch_size = len(converted_texts[:50]),\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = 1.0,\n",
    "                                   learning_rate = 0.0,\n",
    "                                   beam_width = 5,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   inference_targets = True,\n",
    "                                   pretrained_embeddings_path = None)#pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path =  './models/sogou/my_model',\n",
    "                         targets = converted_summaries[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 11917
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1526243456128,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "JtB2kNIWmw2j",
    "outputId": "b2b34d18-062a-4ea0-e48f-29ed6c3fe123",
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e6b2145d14d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# show results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m summarizer_model_utils.sample_results(preds,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                       \u001b[0mind2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       \u001b[0mword2ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mconverted_summaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "summarizer_model_utils.sample_results(preds,\n",
    "                                      ind2word,\n",
    "                                      word2ind,\n",
    "                                      converted_summaries[:50],\n",
    "                                      converted_texts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_bcG5CPmw2m"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Generally I am really impressed by how well the model works. \n",
    "We only used a limited amount of data, trained it for a limited amount of time and used nearly random hyperparameters and it still delivers good results. \n",
    "\n",
    "However, we are clearly overfitting the training data and the model does not perfectly generalize.\n",
    "Sometimes the summaries the model creates are good, sometimes bad, sometimes they are better than the original ones and sometimes they are just really funny.\n",
    "\n",
    "\n",
    "Therefore it would be really interesting to scale it up and see how it performs. \n",
    "\n",
    "To sum up, I am impressed by seq2seq models, they perform great on many different tasks and I look foward to exploring more possible applications. \n",
    "(speech recognition...)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "summarizer_amazon_reviews.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python_defaultSpec_1599824957121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}