{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tk5ROC6mw0f"
   },
   "source": [
    "# Text Summarization of Amazon reviews\n",
    "\n",
    "In this notebook I will write summaries with the help of my Seq2Seq model in Summarizer.py.\n",
    "\n",
    "The model works impressively well in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nRequirement already satisfied: nltk in /root/anaconda3/envs/t/lib/python3.6/site-packages (3.5)\nRequirement already satisfied: regex in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (2020.7.14)\nRequirement already satisfied: joblib in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (0.16.0)\nRequirement already satisfied: click in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: tqdm in /root/anaconda3/envs/t/lib/python3.6/site-packages (from nltk) (4.48.2)\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B_ULQF2smw0h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from collections import Counter\n",
    "\n",
    "import Summarizer\n",
    "import summarizer_data_utils\n",
    "import summarizer_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.14.0\n"
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XBIbvs4mw0m"
   },
   "source": [
    "## The data\n",
    "\n",
    "\n",
    "The data we will be using with is a dataset from Kaggle, the Amazon Fine Food Reviews dataset.  \n",
    "It contains, as the name suggests, 570.000 reviews of fine foods from Amazon and summaries of those reviews. \n",
    "Our aim is to input a review (Text column) and automatically create a summary (Summary colum) for it.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NYVncFKmw0n"
   },
   "source": [
    "### Reading and exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4894,
     "status": "ok",
     "timestamp": 1526227108183,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "2OiSxpApmw0o",
    "outputId": "98a255ad-248e-4f1e-b43f-1c8d384fd453"
   },
   "outputs": [],
   "source": [
    "# load csv file using pandas.\n",
    "#file_path = './Reviews.csv'\n",
    "#data = pd.read_csv(file_path)\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rElWMbT2mw0t",
    "outputId": "08f4b9ee-8c78-4f3b-c986-f7f3dd2ff248"
   },
   "outputs": [],
   "source": [
    "# we will only use the last two columns Summary (target) and Text (input).\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1526227113630,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "N9bHztjpmw0x",
    "outputId": "aa4ce93d-efde-4ebb-bc3e-7b03d477a5e4"
   },
   "outputs": [],
   "source": [
    "# check for missings --> got some in summary drop those. \n",
    "# 26 are missing, so we will drop those!\n",
    "#data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "boMCgsgTmw00"
   },
   "outputs": [],
   "source": [
    "# drop row, if values in Summary is missing. \n",
    "#data.dropna(subset=['Summary'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1526227125421,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ESv4XLgQmw03",
    "outputId": "0ca5d3f7-7efc-46dc-bf8e-bb5803c6376c"
   },
   "outputs": [],
   "source": [
    "# only summary and text are useful for us.\n",
    "#data = data[['Summary', 'Text']]\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BjmIGbXtmw08"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nraw_texts = []\\nraw_summaries = []\\n\\nfor text, summary in zip(data.Text, data.Summary):\\n    if 100< len(text) < 150:\\n        raw_texts.append(text)\\n        raw_summaries.append(summary)\\n        '"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# we will not use all of them, only short ones and ones of similar size. \n",
    "# choosing the ones that are of similar length makes it easier for the model to learn.\n",
    "'''\n",
    "raw_texts = []\n",
    "raw_summaries = []\n",
    "\n",
    "for text, summary in zip(data.Text, data.Summary):\n",
    "    if 100< len(text) < 150:\n",
    "        raw_texts.append(text)\n",
    "        raw_summaries.append(summary)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1560,
     "status": "ok",
     "timestamp": 1526227148045,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "t5JBoyqKmw0_",
    "outputId": "150af52c-a0af-4eec-f0ff-399ec33e434d"
   },
   "outputs": [],
   "source": [
    "#len(raw_texts), len(raw_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tMsDeec4mw1F",
    "outputId": "de46147e-d2c4-40e5-9a0a-22f27ac360fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for t, s in zip(raw_texts[:5], raw_summaries[:5]):\n",
    " #   print('Text:\\n', t)\n",
    "  #  print('Summary:\\n', s, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-CyKX1gmw1J"
   },
   "source": [
    "### Clean and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47800,
     "status": "ok",
     "timestamp": 1526227313932,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "4CLoTqyzmw1K",
    "outputId": "2fcb8327-e714-48e2-fca1-6bf39b8e6411",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nimport nltk\\nnltk.download('punkt')\\nprocessed_texts, processed_summaries, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\\n    raw_texts,\\n    raw_summaries,\\n    keep_most=False\\n)\\nprint(processed_texts[:3])\\nprint(processed_summaries[:3])\\nprint(len(words_counted))\\nprint(words_counted[:5])\\n\""
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# the function gives us the option to keep_most of the characters inisde the texts and summaries, meaning\n",
    "# punctuation, question marks, slashes...\n",
    "# or we can set it to False, meaning we only want to keep letters and numbers like here.\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "processed_texts, processed_summaries, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\n",
    "    raw_texts,\n",
    "    raw_summaries,\n",
    "    keep_most=False\n",
    ")\n",
    "print(processed_texts[:3])\n",
    "print(processed_summaries[:3])\n",
    "print(len(words_counted))\n",
    "print(words_counted[:5])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yquphHYJmw1R",
    "outputId": "cd9ad917-33da-4294-eb41-f3b0abb967e9",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.632 seconds.\nPrefix dict has been built successfully.\n[('的', 43987), ('１', 20805), ('０', 19880)]\n[['南', '都', '讯', '\\u3000', '记者', '刘凡', '\\u3000', '周昌', '和', '\\u3000', '任笑', '一', '\\u3000', '继', '推出', '日票', '后', '深圳', '今后', '将', '设', '地铁', 'Ｖ', 'Ｉ', 'Ｐ', '头等', '车厢', '设', '坐票', '制', '昨日', '《', '南', '都', 'Ｍ', 'Ｅ', 'Ｔ', 'Ｒ', 'Ｏ', '》', '创刊', '仪式', '暨', '２', '０', '１', '２', '年', '深港', '地铁', '圈', '高峰论坛', '上', '透露', '在', '未来', '的', '１', '１', '号线', '上将', '增加', '特色', '服务', '满足', '不同', '消费', '层次', '的', '乘客', '的', '不同', '需求', '如', '特设', '行李架', '的', '车厢', '和', '买', '双倍', '票', '可', '有', '座位', '坐', '的', 'Ｖ', 'Ｉ', 'Ｐ', '车厢', '等', '\\ue40c', '论坛', '上', '深圳市政府', '副', '秘书长', '轨道交通', '建设', '办公室', '主任', '赵鹏林', '透露', '地铁', '未来', '的', '方向', '将', '分等级', '满足', '不同', '层次', '的', '人', '的', '需求', '提供', '不同', '层次', '的', '有', '针对', '的', '服务', '其中', '包括', '一些', '档次', '稍微', '高', '一些', '的', '服务'], ['同心县', '地处', '宁夏', '中部', '干旱', '带', '的', '核心区', '\\u3000', '冬寒', '长', '春暖迟', '夏热', '短', '秋凉', '早', '干旱', '少雨', '蒸发', '强烈', '风大沙', '多', '主要', '自然灾害', '有', '沙尘暴', '干热风', '霜冻', '冰雹', '等', '其中', '以', '干旱', '危害', '最为', '严重', '\\ue40c', '由于', '生态环境', '的', '极度', '恶劣', '导致', '农村', '经济', '发展缓慢', '人民', '群众', '生产', '生活', '水平', '低下', '靠天吃饭', '的', '被动局面', '依然', '存在', '同心', '又', '是', '国家级', '老', '少', '边', '穷县', '之一', '…', '［', '详细', '］'], ['不满', '一岁', '的', '永康', '是', '个', '饱经', '病痛', '折磨', '的', '孩子', '２', '０', '１', '１', '年', '７', '月', '５', '日', '出生', '的', '他', '患有', '先天性', '心脏病', '疝气', '一', '出生', '便', '被遗弃', '２', '０', '１', '２', '年', '１', '月', '８', '日', '才', '５', '个', '月', '大', '的', '永康', '被', '发现', '呼吸困难', '随后', '送往', '医院', '进行', '抢救', '治疗', '病情', '稳定', '后于', '１', '月', '２', '８', '日', '出院', '\\ue40c', '２', '０', '１', '２', '年', '２', '月', '１', '３', '号', '永康', '在', '思源', '焦点', '公益', '基金', '的', '帮助', '下', '在', '医院', '接受', '手术', '治疗', '术后', '仅', '８', '天', '永康', '突发', '右侧', '腹股沟', '斜', '疝', '嵌顿', '及', '肠梗阻', '又', '再次', '进行', '抢救', '治疗', '术后', '进', '重症', '监护室', '３', '月', '７', '日', '几经', '病痛', '折磨', '的', '永康', '终于', '康复', '出院', '目前', '他', '的', '病情', '已经', '稳定']]\n[['深圳', '地铁', '将', '设立', 'Ｖ', 'Ｉ', 'Ｐ', '头等', '车厢', '\\u3000', '买', '双倍', '票', '可享', '坐票'], ['中国', '西部', '是', '地球', '上', '主要', '干旱', '带', '之一', '妇女', '是', '当地', '劳动力', '．', '．', '．'], ['思源', '焦点', '公益', '基金', '救助', '孩子', '永康']]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nprocessed_content, processed_title, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\\n    content,\\n    title,\\n    keep_most=False\\n)'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#for t,s in zip(processed_texts[:5], processed_summaries[:5]):\n",
    " #   print('Text\\n:', t, '\\n')\n",
    "  #  print('Summary:\\n', s, '\\n\\n\\n')\n",
    "\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow.keras as keras\n",
    "max_features=10000\n",
    "maxlen=300\n",
    "dt=pd.read_csv(\"sgnewfull.csv\",sep='\\t',skiprows=1,names=['title','content'],nrows=10000)\n",
    "#按行读取文件，返回文件的行字符串列表,读取stopwords.dat\n",
    "def read_file(file_name):\n",
    "    fp = open(file_name, \"r\", encoding=\"utf-8\")\n",
    "    content_lines = fp.readlines()\n",
    "    fp.close()\n",
    "    #去除行末的换行符，否则会在停用词匹配的过程中产生干扰\n",
    "    for i in range(len(content_lines)):\n",
    "        content_lines[i] = content_lines[i].rstrip(\"\\n\")\n",
    "    return content_lines\n",
    "def fenci(selist):\n",
    "    stopwords = read_file(\"stopwords.dat\")#读取停用词\n",
    "    l=[]\n",
    "    for i in selist:\n",
    "        k=[]\n",
    "        seg_list = jieba.cut(i)  # 默认是精确模式\n",
    "        outstr=''\n",
    "        i=0\n",
    "        for word in seg_list:  #去除停顿词\n",
    "            i+=1\n",
    "            if word not in stopwords:  #如果去除停用词的话，把注释去掉，同时把下面三行加tab\n",
    "                if word != '\\t' and i<150 :  \n",
    "                    k.append(word)\n",
    "        l.append(k)\n",
    "    return l\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features,lower=True)\n",
    "dt['title']=dt['title'].fillna(\"\")\n",
    "dt['content']=dt['content'].fillna(\"\")\n",
    "title=dt['title']\n",
    "#title=[('s'+i+'e')for i in title]\n",
    "\n",
    "content=fenci(list(dt['content']))\n",
    "title=fenci(title)\n",
    "#统计词\n",
    "words=[]\n",
    "for text in content:\n",
    "    for word in text:\n",
    "        words.append(word)\n",
    "for text in title:\n",
    "    for word in text:\n",
    "        words.append(word)\n",
    "words_counted = Counter(words).most_common() \n",
    "print(words_counted[:3])\n",
    "print(content[:3])     \n",
    "print(title[:3])\n",
    "'''\n",
    "processed_content, processed_title, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\n",
    "    content,\n",
    "    title,\n",
    "    keep_most=False\n",
    ")'''\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer.fit_on_texts(content + \n",
    " #                      title)\n",
    "#content=tokenizer.texts_to_sequences(content)\n",
    "#content=keras.preprocessing.sequence.pad_sequences(content,maxlen=maxlen) # padding\n",
    "#title=tokenizer.texts_to_sequences(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34eUnqVQmw1c"
   },
   "source": [
    "### Create lookup dicts\n",
    "\n",
    "We cannot feed our network actual words, but numbers. So we first have to create our lookup dicts, where each words gets and int value (high or low, depending on its frequency in our corpus). Those help us to later convert the texts into numbers.\n",
    "\n",
    "We also add special tokens. EndOfSentence and StartOfSentence are crucial for the Seq2Seq model we later use.\n",
    "Pad token, because all summaries and texts in a batch need to have the same length, pad token helps us do that.\n",
    "\n",
    "So we need 2 lookup dicts:\n",
    " - From word to index \n",
    " - from index to word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1526227336251,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "zwqbTP8jmw1d",
    "outputId": "3788ccc5-bec0-4d32-8885-0698b45d7164",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "80304 80304 0\n"
    }
   ],
   "source": [
    "specials = [\"<EOS>\", \"<SOS>\",\"<PAD>\",\"<UNK>\"]\n",
    "word2ind, ind2word,  missing_words = summarizer_data_utils.create_word_inds_dicts(words_counted,\n",
    "                                                                       specials = specials)\n",
    "print(len(word2ind), len(ind2word), len(missing_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVZ1Qmk9mw1j"
   },
   "source": [
    "### Pretrained embeddings\n",
    "\n",
    "Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy.\n",
    "Here I used two different options. Either we use glove embeddings or embeddings from tf_hub.\n",
    "The ones from tf_hub worked better, so we use those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embeddings_path = './glove.6B.300d.txt'\n",
    "# embedding_matrix_save_path = './embeddings/my_embedding_github.npy'\n",
    "# emb = summarizer_data_utils.create_and_save_embedding_matrix(word2ind,\n",
    "#                                                              glove_embeddings_path,\n",
    "#                                                              embedding_matrix_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61662,
     "status": "ok",
     "timestamp": 1526227413054,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ObE6ggfAmw1o",
    "outputId": "e12b1f22-0fad-4a3d-e934-5fc480b83788"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nembed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\\nemb = embed([key for key in word2ind.keys()])\\n\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    sess.run(tf.tables_initializer())\\n    embedding = sess.run(emb)\\n'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# the embeddings from tf_hub. \n",
    "# embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "'''\n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\n",
    "emb = embed([key for key in word2ind.keys()])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding = sess.run(emb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1526227413774,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ayXi9D7Umw1u",
    "outputId": "7b5b5522-8c21-4c70-a5be-46f6ed2cdbd2"
   },
   "outputs": [],
   "source": [
    "#embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QoGa9EWdmw11"
   },
   "outputs": [],
   "source": [
    "#np.save('./tf_hub_embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QV1HB3zzmw12"
   },
   "source": [
    "### Convert text and summaries\n",
    "\n",
    "As I said before we cannot feed the words directly to our network, we have to convert them to numbers first of all. This is what we do here. And we also append the SOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NjudfxFPmw13"
   },
   "outputs": [],
   "source": [
    "# converts words in texts and summaries to indices\n",
    "# it looks like we have to set eos here to False\n",
    "processed_texts=content\n",
    "converted_texts, unknown_words_in_texts = summarizer_data_utils.convert_to_inds(processed_texts,\n",
    "                                                                                word2ind,\n",
    "                                                                                eos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1dFsLoAqmw16"
   },
   "outputs": [],
   "source": [
    "processed_summaries=title\n",
    "converted_summaries, unknown_words_in_summaries = summarizer_data_utils.convert_to_inds(processed_summaries,word2ind, eos = True,  sos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2143,
     "status": "ok",
     "timestamp": 1526227545460,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ghATcyE4mw2A",
    "outputId": "06d3f934-7c97-49e2-c358-21bd7552689d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(converted_texts[:3])\n",
    "#print(converted_summaries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1526227550694,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "pSTzMURHmw2E",
    "outputId": "4dc8b405-f811-49f2-b7b3-83495dbf7640",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seems to have worked well. \n",
    "#print( summarizer_data_utils.convert_inds_to_text(converted_texts[0], ind2word),\n",
    "     #  summarizer_data_utils.convert_inds_to_text(converted_summaries[0], ind2word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8b9Nd0zmw2H"
   },
   "source": [
    "## The model\n",
    "\n",
    "Now we can build and train our model. First we define the hyperparameters we want to use. Then we create our Summarizer and call the function .build_graph(), which as the name suggests, builds the computation graph. \n",
    "Then we can train the model using .train()\n",
    "\n",
    "After training we can try our model using .infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2z9xOKzmw2I"
   },
   "source": [
    "### Training\n",
    "\n",
    "We can optionally use a cyclic learning rate, which we do here. \n",
    "I trained the model for 20 epochs and the loss was low then, but we could train it longer and would probably get better results.\n",
    "\n",
    "Unfortunately I do not have the resources to find the perfect (or right) hyperparameters, but these do pretty well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tEItjpP4mw2J"
   },
   "outputs": [],
   "source": [
    "# model hyperparametes\n",
    "num_layers_encoder = 2\n",
    "num_layers_decoder = 2\n",
    "rnn_size_encoder = 128\n",
    "rnn_size_decoder = 128\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "clip = 5\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.001\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 700\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = './tf_hub_embedding.npy'\n",
    "summary_dir = os.path.join('./tensorboard', str('Nn_' + str(rnn_size_encoder) + '_Lr_' + str(learning_rate)))\n",
    "\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1464,
     "status": "ok",
     "timestamp": 1526234914336,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "u8lJ_OI5mw2Q",
    "outputId": "1c06bc51-01eb-4a68-b4ca-38d56a4a2a76"
   },
   "outputs": [],
   "source": [
    "#len(converted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1526234915582,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "w_VDuiHyQK84",
    "outputId": "9bc17a2e-837b-41bd-a40d-0f116e143d8f"
   },
   "outputs": [],
   "source": [
    "d=round(len(converted_summaries)*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from numba import cuda\n",
    "#cuda.select_device(0)\n",
    "#cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85881
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8531236,
     "status": "error",
     "timestamp": 1526243447242,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "E0BX6Z7Kmw2T",
    "outputId": "e734411e-2fbc-4960-e2aa-fb98108c4576",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /root/my/summarizer_model_utils.py:117: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n\nWARNING:tensorflow:From /root/my/summarizer_model_utils.py:118: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:108: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:126: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /root/my/Summarizer.py:615: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:157: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From /root/my/Summarizer.py:158: The name tf.nn.rnn_cell.DropoutWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.DropoutWrapper instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:254: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe9ef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c2852198>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/my/Summarizer.py:259: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:383: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b411fef0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f94b4059fd0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95c1fe9e48>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95b44558d0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f95c1fe96a0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b465a6d8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b449add8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py:107: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.random.categorical` instead.\nWARNING:tensorflow:From /root/my/Summarizer.py:221: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:104: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\nGraph built.\n-------------------- Epoch 0 of 10 --------------------\n"
    },
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dynamic_seq2seq/Adam/update/_182]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3':\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/../pyvsc-run-isolated.py\", line 24, in <module>\n    runpy.run_path(module, run_name=\"__main__\")\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/kernel_prewarm_starter.py\", line 31, in <module>\n    runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 208, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-27-cb70788c0a65>\", line 23, in <module>\n    summarizer.build_graph()\n  File \"/root/my/Summarizer.py\", line 103, in build_graph\n    self.add_seq2seq()\n  File \"/root/my/Summarizer.py\", line 197, in add_seq2seq\n    encoder_state)\n  File \"/root/my/Summarizer.py\", line 302, in build_decoder\n    scope=decoder_scope\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in dynamic_decode\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in <lambda>\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1205, in stack\n    return self._implementation.stack(name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 309, in stack\n    return self.gather(math_ops.range(0, self.size()), name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 323, in gather\n    element_shape=element_shape)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6705, in tensor_array_gather_v3\n    element_shape=element_shape, name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dynamic_seq2seq/Adam/update/_182]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb70788c0a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                  \u001b[0mconverted_summaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                  \u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverted_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                  validation_targets=converted_summaries[d:])\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my/Summarizer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, targets, restore_path, validation_inputs, validation_targets)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;31m# run training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my/Summarizer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, inputs, targets, epoch)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     _, train_loss = self.sess.run([self.train_op, self.train_loss],\n\u001b[0;32m--> 523\u001b[0;31m                                                   feed_dict=fd)\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnbatches\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dynamic_seq2seq/Adam/update/_182]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[25,64,722908] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3 (defined at /root/my/Summarizer.py:302) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3':\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/../pyvsc-run-isolated.py\", line 24, in <module>\n    runpy.run_path(module, run_name=\"__main__\")\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/.vscode-server/extensions/ms-python.python-2020.8.108011/pythonFiles/vscode_datascience_helpers/kernel_prewarm_starter.py\", line 31, in <module>\n    runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 208, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \"/root/anaconda3/envs/t/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-27-cb70788c0a65>\", line 23, in <module>\n    summarizer.build_graph()\n  File \"/root/my/Summarizer.py\", line 103, in build_graph\n    self.add_seq2seq()\n  File \"/root/my/Summarizer.py\", line 197, in add_seq2seq\n    encoder_state)\n  File \"/root/my/Summarizer.py\", line 302, in build_decoder\n    scope=decoder_scope\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in dynamic_decode\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in <lambda>\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1205, in stack\n    return self._implementation.stack(name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 309, in stack\n    return self.gather(math_ops.range(0, self.size()), name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 323, in gather\n    element_shape=element_shape)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6705, in tensor_array_gather_v3\n    element_shape=element_shape, name=name)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# build graph and train the model \n",
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   save_path='./models/sogou/my_model',\n",
    "                                   mode='TRAIN',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   batch_size = 32,\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = keep_probability,\n",
    "                                   learning_rate = learning_rate,\n",
    "                                   max_lr=max_lr,\n",
    "                                   learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                                   learning_rate_decay = learning_rate_decay,\n",
    "                                   epochs = epochs,\n",
    "                                   pretrained_embeddings_path = None, #pretrained_embeddings_path,\n",
    "                                   use_cyclic_lr = use_cyclic_lr,\n",
    "                                   summary_dir = None)#summary_dir)           \n",
    "\n",
    "summarizer.build_graph()\n",
    "summarizer.train(converted_texts[:d], \n",
    "                 converted_summaries[:d],\n",
    "                 validation_inputs=converted_texts[d:],\n",
    "                 validation_targets=converted_summaries[d:])\n",
    "\n",
    "\n",
    "# hidden training output.\n",
    "# both train and validation loss decrease nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5Hqzvocmw2W"
   },
   "source": [
    "### Inference\n",
    "Now we can use our trained model to create summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4607,
     "status": "ok",
     "timestamp": 1526243454761,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ljN9a1hemw2Y",
    "outputId": "f60102af-44f0-4c45-8ba3-2548e5af0a4c",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /root/my/summarizer_model_utils.py:117: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n\nWARNING:tensorflow:From /root/my/summarizer_model_utils.py:118: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:108: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:126: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /root/my/Summarizer.py:615: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:157: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:From /root/my/Summarizer.py:158: The name tf.nn.rnn_cell.DropoutWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.DropoutWrapper instead.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:254: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f33390>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/my/Summarizer.py:259: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n\nWARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From /root/my/Summarizer.py:383: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b81976a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b81976a0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b81976a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b81976a0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f95b813ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f95b813ccf8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f95b813ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f95b813ccf8>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95f8094a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95f8094a58>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95f8094a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f95f8094a58>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f969c5e00f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f969c5e00f0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f969c5e00f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f969c5e00f0>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f96b4f55a20>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b8197128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b8197128>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b8197128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f95b8197128>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f96a808ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f96a808ae80>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f96a808ae80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f96a808ae80>>: AttributeError: module 'gast' has no attribute 'Index'\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:985: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From /root/my/Summarizer.py:104: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\nGraph built.\nWARNING:tensorflow:From /root/anaconda3/envs/t/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\nINFO:tensorflow:Restoring parameters from ./models/sogou/my_model\nDone.\n"
    }
   ],
   "source": [
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   './models/sogou/my_model',\n",
    "                                   'INFER',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   batch_size = len(converted_texts[:50]),\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = 1.0,\n",
    "                                   learning_rate = 0.0,\n",
    "                                   beam_width = 5,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   inference_targets = True,\n",
    "                                   pretrained_embeddings_path = None)#pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path =  './models/sogou/my_model',\n",
    "                         targets = converted_summaries[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 11917
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1526243456128,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "JtB2kNIWmw2j",
    "outputId": "b2b34d18-062a-4ea0-e48f-29ed6c3fe123",
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n南 都 讯 　 记者 刘凡 　 周昌 和 　 任笑 一 　 继 推出 日票 后 深圳 今后 将 设 地铁 Ｖ Ｉ Ｐ 头等 车厢 设 坐票 制 昨日 《 南 都 Ｍ Ｅ Ｔ Ｒ Ｏ 》 创刊 仪式 暨 ２ ０ １ ２ 年 深港 地铁 圈 高峰论坛 上 透露 在 未来 的 １ １ 号线 上将 增加 特色 服务 满足 不同 消费 层次 的 乘客 的 不同 需求 如 特设 行李架 的 车厢 和 买 双倍 票 可 有 座位 坐 的 Ｖ Ｉ Ｐ 车厢 等  论坛 上 深圳市政府 副 秘书长 轨道交通 建设 办公室 主任 赵鹏林 透露 地铁 未来 的 方向 将 分等级 满足 不同 层次 的 人 的 需求 提供 不同 层次 的 有 针对 的 服务 其中 包括 一些 档次 稍微 高 一些 的 服务\n\nActual Summary:\n深圳 地铁 将 设立 Ｖ Ｉ Ｐ 头等 车厢 　 买 双倍 票 可享 坐票\n\nCreated Summary:\n中国 中国 中国 中国 　 　 　 称 称\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n同心县 地处 宁夏 中部 干旱 带 的 核心区 　 冬寒 长 春暖迟 夏热 短 秋凉 早 干旱 少雨 蒸发 强烈 风大沙 多 主要 自然灾害 有 沙尘暴 干热风 霜冻 冰雹 等 其中 以 干旱 危害 最为 严重  由于 生态环境 的 极度 恶劣 导致 农村 经济 发展缓慢 人民 群众 生产 生活 水平 低下 靠天吃饭 的 被动局面 依然 存在 同心 又 是 国家级 老 少 边 穷县 之一 … ［ 详细 ］\n\nActual Summary:\n中国 西部 是 地球 上 主要 干旱 带 之一 妇女 是 当地 劳动力 ． ． ．\n\nCreated Summary:\n２ ２ ２ ２ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n不满 一岁 的 永康 是 个 饱经 病痛 折磨 的 孩子 ２ ０ １ １ 年 ７ 月 ５ 日 出生 的 他 患有 先天性 心脏病 疝气 一 出生 便 被遗弃 ２ ０ １ ２ 年 １ 月 ８ 日 才 ５ 个 月 大 的 永康 被 发现 呼吸困难 随后 送往 医院 进行 抢救 治疗 病情 稳定 后于 １ 月 ２ ８ 日 出院  ２ ０ １ ２ 年 ２ 月 １ ３ 号 永康 在 思源 焦点 公益 基金 的 帮助 下 在 医院 接受 手术 治疗 术后 仅 ８ 天 永康 突发 右侧 腹股沟 斜 疝 嵌顿 及 肠梗阻 又 再次 进行 抢救 治疗 术后 进 重症 监护室 ３ 月 ７ 日 几经 病痛 折磨 的 永康 终于 康复 出院 目前 他 的 病情 已经 稳定\n\nActual Summary:\n思源 焦点 公益 基金 救助 孩子 永康\n\nCreated Summary:\n中国 中国 ２ ２ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n就 废弃 茶叶 被 转手 事件 发声明  本报讯 记者 刘俊 　 我们 也 是 受害者 昨日 有 媒体报道 称 康师傅 的 废弃 茶叶 被 转手 卖 给 不良 商家 冒充 名茶 流入 市场 康师傅 的 一位 联系人 这样 说 康师傅 昨日 晚间 发出 声明 表示 生产 废料 处理 商 作出 了 　 不良行为 公司 方面 除 表达 严正 关注 的 立场 外 也 已经 配合 政府 有关 单位 进行 调查  中止 与 生产 废料 厂商 的 合同  康师傅 在 给 本报 的 声明 中 确认 经查 与 康师傅 签订 生产 废料 处理 合同 之 厂商 为 吉安 三石 饲料 商行 且系 通过 公开招标 程序 取得 合同 承揽 资格 并 签有 保证 透过 合法 渠道 处理 康师傅 生产 废料 的 承诺 昨日 媒体报道 的 内容 显示 吉安 三石\n\nActual Summary:\n康师傅 回应 转卖 废弃 茶叶 下 家 承诺 用 废料 做 枕头\n\nCreated Summary:\n《 中国 中国 中国 中国 中国 中国 　 　\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n· 奖励 办法 率先 提交 的 前 １ ０ ０ 个 创意 项目 经 评估 可 优先 资助 实施  · 咨询电话 ０ １ ０ － ６ ７ ７ ８ ４ ７ １ ０ ０ １ ０ － ６ ７ ７ ８ ４ ７ ２ ０  · 报名 方式 先 下载 报名表 填写 完整 网上 直接 上传 项目 概述 完整 方案 发 邮箱\n\nActual Summary:\n活动 时间\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n全民 健康 生活 方式 行动 ２ ０ ０ ９ 年度 健康 血压 主题 活动 拉开帷幕 ［ 详细 ］\n\nActual Summary:\n全民 健康 生活 方式 　 健康 血压 活动\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ０ ０ ９ 年 ８ 月 《 ２ ０ ０ ９ 中国 慈善 导航 行动 》 第一季 正式 启动 此档 由 Ｃ Ｃ Ｔ Ｖ － １ ２ 《 大家 看法 》 精心制作 的 节目 将 于 ２ ０ ０ ９ 年 ８ 月 １ ７ 日至 ８ 月 ２ ３ 日 每晚 ８ 点整 在 中央电视台 社会 与 法 频道 Ｃ Ｃ Ｔ Ｖ － １ ２ 正式 播出 节目 将 从 １ ０ ０ 多个 ５ ． １ ２ 灾后 重建 资助 项目 中 挑选出 １ ８ 个 优秀 服务项目 参与 电视 评估 及 展示 １ ８ 个 项目 中 究竟 哪些项目 将 得到 千万 基金 的 支持 … 　 ［ 详细 ］\n\nActual Summary:\n５ ． １ ２ 灾后 重建 资助 项目 投票 评选\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ０ １ ２ 年 东风 标致 小桔灯 乡村 小学 图书馆 计划 于 ６ 月 ２ ３ 日 － ２ ９ 日 在 湖北省 武汉市 新洲区 凤凰 镇 郭岗 小学 举行 来自 全国 各地 的 ７ 名 志愿者 带 着 东风 标致 ５ ０ ８ 汽车 捐赠 的 ２ ０ ０ ０ 本 新书 建立 起有 一所 小桔灯 乡村 小学 图书馆  武汉市 新洲区 不仅 是 革命 老区 也 是 全国 有名 的 建筑 之 乡 这里 的 人 大部分 都 外出 从事 建筑业 导致 这里 的 留守 儿童 比例 其高 郭岗 小学 现有 学生 １ ３ ６ 名 含 教学点 一处 其中 住校 学生 ９ ８ 名 全校 ９ ０ ％ 左右 的 学生 都 是 留守 儿童  在 活动 举行 之前 学校 图书室 里 摆满 了 陈旧 的 教科书 这里\n\nActual Summary:\n以书 为友 知行合一 — ２ ０ １ ２ 年 小 桔灯 湖北 站\n\nCreated Summary:\n中国 中国 １ １ １ １ １ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n１ 纪念 汶川 地震 １ 周年  ２ 纪念 ５ 月 １ ２ 日 国家 首个 防灾 减灾 日  ３ 纪念 索尔 弗利 诺 战役 １ ５ ０ 周年  ４ 纪念 红十字会 与 红新月会 国际 联合会 成立 ９ ０ 周年  ５ 纪念 中国红十字会 成立 １ ０ ５ 周年  ６ 纪念 国际 人道法 签订 ６ ０ 周年\n\nActual Summary:\n博爱 周 活动 时间\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n世间 本 没有 歧视 歧视 源自 于 人 的 内心 活动 以爱之名 ２ ０ ０ ９ 年 中国 艾滋病 反 歧视 主题 创意 大赛 开幕 让 爱 在 高校 流动 ［ 详细 ］\n\nActual Summary:\n艾滋病 反 歧视 创意 大赛\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ９ 月 ２ ２ 日 与 阿坝州 及 茂县 各方 人士 商议 在 四川省阿坝藏族羌族自治州 组建 羌族 少儿 合唱团  ３ １ ２ 月 ３ － ５ 日 阿坝州 茂县 凤仪 小学 凤仪 镇 小学 选拔 团员  ４ ２ ０ ０ ９ 年 １ 月 ９ 日 羌族 少儿 合唱团 计划 在 深圳 启动\n\nActual Summary:\n金葵花 羌族 少儿 合唱团 公益活动\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n从 汽车 发明 到 现在 平均 每天 因 事故 死亡 ３ ３ ０ ０ 人 左右 相当于 １ ０ 架 大型 客机 坠毁 这些 事故 中 ９ ５ ％ 的 事故 都 与 人员 失误 有关 只有 ４ ％ 的 事故 是 纯 机械故障  为此 ２ １ 世纪 经济 报道 与 广汽 丰田 汽车 有限公司 联合 发起 文明 驾驶 理念 普及 行动 邀请 全国 范围 内 的 社会 人士 专业 机构 及 各 高校 参与 以 文明 驾驶 为 主题 　 针对 各种 不 文明 驾驶 行为 通过 公益 视觉 设计 的 方式 进行 普罗 教育  视频 入围 作品 展示 如下  一 ．  留住 童真  作品名称 留住 童真  作者 崔 艺文 ／ 广东 商学院  创意 说明 以 简单 可爱 的 剪纸\n\nActual Summary:\n视频 征集 入围 短片\n\nCreated Summary:\n视频 报告 报告\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n甘肃 首批 １ ６ 所 偏远 山区 学校 中 有 １ ５ 所 学校 的 １ １ ９ ７ 名 困难 学生 有望 获得 爱心 运动鞋  截至 目前 已 从 平凉 白银 定西 临夏 和 兰州 四地 共 征集 到 １ ９ 所 贫困 受捐 学校  本报讯 ６ 月 １ ３ 日 上午 从 给 孩子 送双 运动鞋 公益 行动 北京 组委会 传来 消息 称 经 初步 审查 由 本报 报送 的 省内 首批 １ ６ 所 偏远 山区 学校 中 有 １ ５ 所 学校 的 １ １ ９ ７ 名 困难 学生 有望 获得 爱心 运动鞋  给 孩子 送双 运动鞋 公益 行动 由 中华 少年儿童 慈善 救助 基金会 崔永元 我 的 长征 团队 及 搜狐网 联合 发起 自从 ５ 月 ２ ８ 日 正式 上线\n\nActual Summary:\n１ ５ 所 学校 过 初选 　 甘肃 １ １ ９ ７ 名 山里娃 有望 穿 上 新鞋\n\nCreated Summary:\n１ １ １ １ １ １ １ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n野草 的 经费 ６ ０ ％ 来自 基金会 赞助 另外 ４ ０ ％ 是 通过 策划 项目 赚来 的 ２ ０ ０ ６ 年 一年 野草 所有人 一分钱 也 没发 ２ ０ ０ ７ 年 发下 了 ５ ０ ％ 的 象征性 工资 ２ ０ ０ ８ 年 野草 实现 了 全额 发工资 算 成都 市民 工资 的 中下等 水平 ［ 详细 ］\n\nActual Summary:\n２ ０ １ ０ 地球 一 小时\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n６ 月 １ ９ 日 《 ２ ０ １ ２ 年度 中国 爱心 城市 公益活动 新闻 发布会 》 在京举行 中华 社会 救助 基金会 理事长 许嘉璐 到 会 讲话 基金会 高级顾问 朱发忠 全国 老龄 办 副 主任 朱勇 民政部 社会 救助 司 助理 巡视员 周萍 中华 社会 救助 基金会 副理事长 耿 志远 重庆市 民政局 巡视员 谭 明政 晋江市 人大常委会 主任 陈健倩 以及 １ ０ 余个 省 市 自治区 民政局 领导 及 四十多家 媒体 参加 了 发布会  中华 社会 救助 基金会 秘书长 时正 新 介绍 本年度 中国 爱心 城市 公益活动 将 以 爱心 城市 宣传 孤老 关爱 救助 项目 及 第二届 中国 爱心 城市 大会 为 主要 内容 重庆市 呼和浩特市 长沙市 太原市 蚌埠市 南昌市 汕头市 沧州市 晋江市 及 遵化市 将会 积极 参加\n\nActual Summary:\n２ ０ １ ２ 年 ＂ 中国 爱心 城市 ＂ 公益活动 举行 新闻 发布会\n\nCreated Summary:\n１ １ １ １ １ １ １ １ １ １ １ １ １ １ １ １ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n信息 时报讯 　 记者 　 何小敏 　 李星慧 南山 奶粉 ５ 批次 含强 致癌物 前日 深夜 广州市 工商局 在 官网 公布 了 近期 对 市面上 乳制品 及 含乳 食品 的 抽检 结果 本次 共 抽取 样品 １ ２ ３ １ 批次 实物 质量 合格 １ ２ ０ ４ 批次 合格率 为 ９ ７ ． ８ １ ％ 值得注意 的 是 知名品牌 光明 奶油 南山 奶粉 及 本土 沙湾 姜汁 撞奶 均 在 不 合格 之 列 爱馨 多 羊奶粉 更是 两月 三登 黑榜  南山 奶粉 ５ 批次 含强 致癌物  本次 抽检 中 问题 最为 严重 的 当属 南山 奶粉 工商部门 表示 共 抽检 该 品牌 奶粉 ５ 个 批次 结果 全部 含有 强 致癌性 物质 黄曲霉 毒素 Ｍ １ 问题 奶粉 由 湖南 长沙 亚华 乳业 有限公司 生产\n\nActual Summary:\n南山 奶粉 ５ 批次 含强 致癌物 　 光明 奶油 上 黑榜\n\nCreated Summary:\n１ １ １ １ １ １ １ １ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n山东 日照 安康 家园 安置 点 集中 了 来自 四川 灾区 的 ５ ２ ２ 名 孩子 其中 ３ ３ ８ 人为 地震 孤儿 在 六一 儿童节 到来之际 为了 让 更 多 的 爱心 人士 帮助 他们 度过 一个 愉快 的 六一 中国儿童少年基金会 与 搜狐 公益 搜狐 社区 开展 因为 有 你 爱 驻 我家 — — 六一 娃娃 礼物 计划 ［ 详细 ］\n\nActual Summary:\n因为 有 你 爱 驻 我家 　 － 　 六一 娃娃 礼物 计划\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n７ 月 ２ 日 深圳市 法制办 在 其 网站 公布 了 《 深圳经济特区 社会 救助 条例 》 以下 简称 《 条例 》 再次 公开 征求 社会 意见 此次 条例 增加 了 流浪 乞讨 人员 生活 救助 和 灾民 生活 救助 两章 内容 根据 规定 救助站 应 向 乞讨 人员 提供 必需 的 食物 住处 以及 医疗 救治 等等 同时 灾民 每人每天 可 领取 ２ ０ 元 的 基本 生活费 投靠 亲友 解决 住宿 的 每人每天 可 领取 ５ ０ 元 补助  对 职业 行乞 者 进行 救助 教育 劝返 和 异地 分流 安置  《 条例 》 规定 对 流浪 乞讨 人员 实行 分类 救助 管理  一 将 自身 无力解决 食宿 无 亲友 投靠 又 不 享受 最低 生活 保障 正在 流浪 乞讨 度日\n\nActual Summary:\n深圳 拟 分类 救助 流浪 乞讨者 　 将 劝返 职业 乞讨 人员\n\nCreated Summary:\n中国 ２ ２ ２ ２ ２ ２ ２ ２ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n目前 这些 数以千计 的 在 英 马来西亚 华人 既 丧失 了 马来西亚 国籍 又 无法 取得 英国 国籍 成为 无国籍 人球 他们 不仅 无权 在 英 工作 甚至 因 无国籍 连 结婚 等 基本 人权 都 无法 保障 ． ． ． ［ 详细 ］\n\nActual Summary:\n国籍 保障 公民 基本权利\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n小区 直饮水 售 水机 必须 要 有 专人 进行 维护 管理 否则 出水 水质 是 得不到 保障 的  分别 是 微生物 指标 毒理学 指标 放射性 指标 感官 性状 指标\n\nActual Summary:\n联合 主办 清华大学 环境 学院 　 中国 水网\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ０ １ ２ 年 ７ 月 １ ６ 日 上午 杜甫 江阁 附近 一位 ７ ２ 岁 的 老人 在 风雨 中 被 儿 活祭 上午 １ ０ 点 记者 赶到 现场 时 暴雨 中 长廊 围坐着 几十人 一位 头发 花白 的 老人 披 着 一件 蓝色 雨衣 跪 坐在 地上 身前 搭 着 祭祀 的 台子 摆放着 食物 和 香烛 老人 面前 两个 头包 白布 女子 正在 捧 香 祭拜 其他 几人 则 在 发传单 给 围观 的 市民 介绍 老人 的 故事 ７ 月 １ ７ 日 国际 在线  父亲 还 活着 却 被 儿女 安排 接受 祭祀 享受 食物 香烛 的 供奉 世上 哪有 如此 残酷 的 祭拜 这 让 人情 何以堪 这 一幕 对 公众 来说\n\nActual Summary:\n徐 大发 活祭 老父 求 救助 是 损德 的 恶 炒\n\nCreated Summary:\n中国 中国 中国 中国 中国 中国 　 　 　 称 称\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n导读 从 ７ 月 中旬 开始 财经频道 倾 各 栏目 之力 联合 出击 聚焦 水流 困局 希望 通过 一个个 极端 案例 揭示 出 水流 背后 的 利益冲突\n\nActual Summary:\n聚焦 水流 困局 视频 播报\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n头枕 砸 窗 靠不住 　 汽车 逃生 器 保险  继 头枕 砸 窗 被 众多 汽车 专家 和 媒体 证实 为 不靠 谱 的 逃生 方法 后 昨天 微博上 又 有 一则 消息 称 原来 头枕 并 不是 拿来 砸 的 而是 拿来 撬 车窗 的  相关 专家 昨天 表示 这一 方法 同样 不靠 谱 撬窗 需有 缝隙 一般 车子 未必 有 头枕 金属杆 顶部 可能 是 平 的 根本 插 不 进去 保险 起 见 还是 专用 的 逃生 锤 比较 好  文 ／ 记者 肖 欢欢 　 实习生 陈玉洁 　 除 署名 外  北京 暴雨 后 网上 流传 一个 头枕 撬开 车窗 的 视频 视频 中 一位 日本 美眉 起初 用 头枕 金属杆 砸 车窗 无法 砸开 后来 将\n\nActual Summary:\n头枕 砸 窗 撬 窗均 被 证 不靠 谱 　 汽车玻璃 贴膜 更难 砸\n\nCreated Summary:\n《 《 　 　 　 称 称 称 称 称\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ０ ０ ８ 年 ８ 月 的 一个 清晨 Ａ 市 女子 某 医院 大院 的 水淹 地里 一个 熟睡 的 女婴 被 人 发现 出生 仅 两天 的 她 被 父母 遗弃 因为 她 患有 先天性 法式 四联 症 心脏 动脉 导管 未闭 大动脉 异常  随后 女婴 被 福利院 收养 坚强 的 她 安静 地 等待 着 希望 的 出现  ２ ０ １ ２ 年 １ 月 萍萍 来到 北京 进行 治疗 ２ ０ １ ２ 年 ３ 月 ２ ３ 日 在 思源 焦点 公益 基金 的 资助 下 萍萍 在 医院 进行 了 近 １ ０ 小时 的 高强度 手术  虽然 手术 是 成功 的 但 术后 萍萍 并 没有 完全 摆脱 危险 她 的 心脏 只能 借助 体外循环 才能 跳动 目前 坚强 的 萍萍\n\nActual Summary:\n思源 焦点 公益 基金 受助 孩子 坚强 的 萍萍\n\nCreated Summary:\n中国 中国 中国 中国 ２ ２ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n７ 月 １ ０ 日 敬老 爱老 公益 计划 在 京 启动 此次 活动 的 主题 为 ＂ 信为 本 孝 为先 ＂ 由 中信银行 与 《 ２ １ 世纪 经济 报道 》 联合 主办 由 全国 老龄 工作 委员会 办公室 为 指导 单位 启动 仪式 当天 全国 老龄 工作 委员会 办公室 副 主任 吴玉韶 中信银行 副行长 曹彤 广东 二十一 世纪 传媒 股份 有限公司 Ｃ Ｅ Ｏ 沈颢 等 领导 莅临 现场 并 致辞  全国 老龄 工作 委员会 办公室 副 主任 吴玉韶 在讲话中 指出 随着 老年人 口 的 快速 增多 我国 正 加速 进入 老龄 社会 根据 最新 预测 到 ２ ０ １ ３ 年 我国 老年人 口 将 超过 ２ 亿 ２ ０ ２ ６ 年 将 超过 ３ 亿 ２ ０ ３ ７ 年 将 超过 ４ 亿 ２\n\nActual Summary:\n信为 本 孝 为先 　 敬老 爱老 公益 计划 在 京 启动\n\nCreated Summary:\n２ ２ ２ ２ ２ ２ ２ ２ ２ ２ ２ ２ ２ ２ ２\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n中 坤 集团 董事长 黄怒波 在 接受 《 福布斯 》 专访 时 表示 现在 慈善 大多 是 官办 我 比较 抵触 捐钱 好像 还得求 着 他们 一样 我 已 不 信任 官办 的 慈善机构 这帮 人 养肥 了 实在 可恶 计划 是 将 一半 的 家产 捐给 北大 １ ０ 年内 逐步 实施 这个 计划 给 我 ５ － １ ０ 　 年 捐 出去 的 资产 怎么着 也 要 超过 ５ ０ ０ 　 亿元  黄怒波 表示 我 现在 不想 捐给 红十字会 等 官办 慈善机构 我 觉得 他们 的 管理 太 差 已经 不 信任 他们 了 自己 也 不想 设立 基金 所以 就 捐给 北大 我 挺 信任 北大 的 此外 还有 一些 零星 的 捐助 比如 最近 马 洪涛 搞 了 一个\n\nActual Summary:\n黄怒波 １ ０ 年内 捐 ５ ０ ０ 亿 　 不再 信任 官办 慈善机构\n\nCreated Summary:\n２ ２ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n无论是 政协会议 的 一号 提案 还是 各省 区 的 人大代表 分组讨论 低碳 成为 今年 两会 的 最 热门话题 之一  未来 我们 国家 已经 不再 具备 延续 发达国家 高 的 能源 消费 为 支撑 的 工业化 道路 发展 低碳 经济 主要 的 是 … …  首先 是 节能 和 提高 能效 其次 发展 可 再生能源 另外 就是 加强 技术 研发 来 推动 低碳 转型  全民 低碳 别 变成 百姓 低碳  低碳 问题 不是 个人 问题  我们 都 忘记 最该 担起 低碳 责任 的 是 企业 国家 ［ 详细 ］  百姓 节约 　 明星 作秀  百姓 想 节水 越 抠 越 好 明星 号召 用 布袋 一年 减碳 １ ０ 克 ［ 详细 ］  把 低 碳\n\nActual Summary:\n低碳 经济 呼唤 戒除 奢侈 消费\n\nCreated Summary:\n中国 中国 中国 中国 中国\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n［ 提要 ］ 　 《 关于 批转 社会保障 十二五 规划 纲要 的 通知 》 提出 要 落实 医疗保险 关系 转移 接续 办法 实现 医疗保险 缴费 年限 在 各地 互认 累计 合并 计算 这是 配合 社会保险 关系 转移 接续 整体 工作 尤其 是 养老保险 关系 转移 接续 的 重要 举措 《 通知 》 还 提出 积极探索 为 独生子女 父母 无 子女 和 失能 老人 提供 必要 的 养老 服务 补贴 和 老年 护理 补贴 ［ 我 来说 两句 ］ ［ 谈及 公务员 养老保险 办法 ］  新 京报 讯 　 记者 韩宇明 国务院 日前 批转 人 社部 发改委 民政部 财政部 卫生部 社保 基金会 联合 制定 的 《 社会保障 十二五 规划 纲要 》 并 下发 通知 要求 贯彻执行 十二五 期间 我国 将 落实 医疗保险 关系 转移 接续 办法\n\nActual Summary:\n医保 缴费 年限 将 各地 互认 并 累计 　 将 研究 延迟 退休\n\nCreated Summary:\n中国 中国 中国 中国 中国 中国 中国 中国 ２ ２ ２ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n思源 ｂ 焦点 公益 基金 的 工作人员 和 志愿者 一起 将 ９ ２ ２ 双鞋 打包  志愿者 为 ＂ 给 孩子 送双 运动鞋 ＂ 公益 行动 献爱心  搜狐网 总编辑 刘春 捐款 证书及 发票  由 中华 少年儿童 慈善 救助 基金会 崔永元 我 的 长征 团队 及 搜狐 联合 发起 的 给 孩子 送双 运动鞋 公益 行动 官网 ｈ ｔ ｔ ｐ ／ ／ ｓ ｏ ｎ ｇ ｘ ｉ ｅ ． ｓ ｏ ｈ ｕ ． ｃ ｏ ｍ 自 ５ 月 ２ ８ 日 正式 上线 以来 受到 网友 的 极大 关注 网友 通过 微博 电话 等 多种 方式 纷纷表示 愿意 捐赠 在 社会 上 掀起 了 献爱心 的 热潮  给 孩子 送双 运动鞋 公益 行动 同时 还 得到 了 众多 知名人士 的 关注 陈坤 崔永元\n\nActual Summary:\n送双 运动鞋 项目 公示 思源 焦点 基金 首批 捐赠\n\nCreated Summary:\n中国 中国 中国 中国 中国 中国 中国 中国 中国\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n搜狐 公益 频道 与 中国 绿化 基金会 联合 发起 的 搜狐 网友 金山岭 植下 许愿树 公益活动 顺利开展 网上 报名 参加 此次 活动 的 有 近百名 来自 社会各界 的 网友 此次 植树 活动 所在地 为 金山岭 山坡 上 坡 陡路 滑 上山 靠 爬 下山 靠 滑 很多 人 认领 多棵 树 也 有 家庭 认领 ５ ６ 棵 树 把 树苗 运 上山 着实 费 了 番 力气 … … 　 ［ 详细 ］ ［ 图集 ］  公民 可以 通过 输入 植树 证书编号 查询 到 所 植树 木 区域 的 生长 情况 树木 照片 实施 定期 更新 植树 生长 情况 查询 网址 为 ｗ ｗ ｗ ． ｇ ｒ ｅ ｅ ｎ ｃ ｈ ｉ ｎ ａ ３ １ ２ ． ｏ ｒ ｇ ． ｃ ｎ … … 　 ［ 详细 ］ \n\nActual Summary:\n社会公众 碳汇 同行 个人 捐助 计划 一人 捐赠 一棵树 争做 绿色 公民\n\nCreated Summary:\n［ 男子 男子 　 　 １ １ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n基本 信任 流失 商业道德 溃败 的 结果 是 以邻为壑 人 与 人 之间 互设 陷阱 你 给 我 吃 地沟油 我 给 你 提供 未 消毒 的 餐具 你 又 吃 了 他 卖 的 问题 奶粉 消毒 餐具 的 杯具 让 这个 社会 更加 缺乏 安全感 ［ 详细 ］  消毒 餐具 不 消毒 而且 卫生 不 合格 的 真相 让 人 触目惊心 目睹 了 消毒 企业 餐具 消毒 现场 的 流程 后 让 人 既 惊心 又 恶心 人们 愤怒 声讨 但 又 无可奈何 很多 人 都 自欺欺人 眼不见为净 也 就 成 了 消费者 息事宁人 的 借口 岂不知 消毒 餐具 的 毒素 正在 潜移默化 的 消磨 着 人们 的 生命 健康 ［ 详细 ］  在 这些 餐具 背后 还\n\nActual Summary:\n从 问题 奶粉 地沟油 到 假 消毒 餐具 更大 的 问题 还 在于 道德 的 溃败 信任 的 失守 ［\n\nCreated Summary:\n我 我 的 的 的 的\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n崔永元 谈 请客 这 是 我 个人 表达 谢意 　 不 代表 任何人  四川 新闻网 北京 ７ 月 ３ ０ 日讯 记者 　 张燕 　 京 港澳 高速 南岗 洼 路段 在 ７ ． ２ １ 特大 自然灾害 中 受灾 严重 当晚 １ ５ ４ 名 在 河西 再生 水厂 工作 的 建筑工人 第一 时间 拿 着 救生圈 麻绳 剪破 铁丝网 钻进 高速公路 救 了 上 百名 危在旦夕 的 司机  ７ 月 ３ ０ 日晚 崔永元 在 京 港澳 高速路 附近 的 一家 饭店 宴请 了 这 １ ５ ４ 名 英勇 救人 的 农民工 朋友 吃饭 在 谈及 请 农民工 吃饭 算不算 做 慈善 时 小 崔说 我 认为 这 就是 我 个人 在 表达 谢意 我 自己 掏腰包 不 代表 任何人  我 为什么 要 以\n\nActual Summary:\n崔永元 请 农民工 吃饭 自己 掏腰包 　 不 代表 任何人\n\nCreated Summary:\n《 大 　 　 １ １ １ １\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n近日 《 环球时报 》 英文版 的 一篇 题为 《 Ｂ ｒ ｉ ｔ 　 Ｂ ｅ ａ ｔ ｅ ｎ 　 Ａ ｆ ｔ ｅ ｒ 　 Ａ ｌ ｌ ｅ ｇ ｅ ｄ 　 Ｓ ｅ ｘ 　 Ａ ｔ ｔ ａ ｃ ｋ 》 被 网友 吵得 沸沸扬扬 惹 来 骂声 不断 这篇 发表 于 ５ 月 １ １ 日 的 文章 描述 老外 在 北京 当街 强奸 中国 女孩 后 有 律师 指出 该 外国人 所 受 刑法 仅为 扣留 １ ５ 天 罚款 ５ ０ ０ 元  有 网友 怒斥 量刑 之少 行为 早已 不是 强奸 少女 而是 强奸 法律  翻译 北京市 康达 律师 事务所 的 刘 文义 音译 律师 表示 这个 男人 将会 面临 １ ５ 天 的 拘留 因为 警方 控告\n\nActual Summary:\n英国 游客 性骚扰 中国 女孩 　 律师 称 可能 轻罚 惹 众怒\n\nCreated Summary:\n中国 中国 中国 中国 中国 中国 ２ ２ ２ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n６ 月底 我们 浙江 传媒 学院 ９ 名 师生 从 杭州 来到 四川省 凉山彝族自治州 甘洛县 尼尔 觉乡 牛吾村 小学 进行 为期 半个 月 的 支教  在 这 所 由 国内 一家 知名 大 公司 在 ２ ０ ０ ７ 年 出资 捐助 建设 的 小学 里 一共 有 ３ ４ ２ 个 孩子 ８ 个班 但 只有 ５ 位 老师 这里 是 彝族 聚集 地区 孩子 们 以 彝语 为 第一 语言 几乎 没有 走出 过 大山 对 外面 的 世界 了解 甚 少  随着 教学质量 的 提升 越来越 多 的 孩子 到 这 所 学校 来 上课 为了 给 新生 腾出 一间 办公室 学校 老师 就要 面临 没有 办公室 的 窘境 但 校长 说 这些 都 没有 问题 他 最大 的 心愿 是 给 孩子 们 修 一个 篮球场\n\nActual Summary:\n浙江 ９ 名 师生 支教 四川 　 凉山 深处 上学 娃 图\n\nCreated Summary:\n中国 中国 ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ０ １ ２ 年 ６ 月 １ ５ 日 是 第 ９ 个 世界 献血者 日 卫生部 表示 未来 一个月 内 全国 各省市 的 血站 将 进行 开放 学校 团体 献血 服务 志愿者 等 可到 血液 中心 参观 血液 检验 加工过程 同时 血液 的 价格 也 将 进行 公示  卫生部医政司 副司长 郭燕红 表示 全国 各省市 血站 开放 月 活动 于 ６ 月 １ ４ 日 正式 启动 活动 从 ６ 月 １ ４ 日至 ７ 月 １ ４ 日 开放 月 活动 主要 是 为了 拉近 血站 和 公众 的 距离  北京市 血液 中心 主任 刘江 表示 开放 月 期间 市 血液 中心 会 组织 医务 工作者 学校 团体 献血 服务 志愿者 等 社会各界 到 血液 中心 参观 血液 的 检验 加工 等 过程 跟随\n\nActual Summary:\n全国 各省市 血站 将 开放 一个月 　 同时 公示 血液 价格\n\nCreated Summary:\n中国 中国 ２ ２ ２ ２ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n联合国 妇女 署 旗舰 报告 指出 女性 取得 的 进展 但是 呼吁 政府 采取 急切 的 行动 在 世界 每个 国家 消除 导致 女性 比 男性 落后 和 弱势 的 不 公正 待遇 《 世界 女性 进展 追求 公正 》 是 联合国 妇女 署 在 ２ ０ １ １ 年初 成立 后 发布 的 第一个 主要 报告 报告 指出 了 女性 进展 的 积极 方面 — — 例如 已有 １ ３ ９ 个 国家 和 地区 将 性别 平等 纳入 宪法 — — 但是 太 多 时候 不管 在 家庭 还是 工作 场合 女性 仍然 面临 不 公正 暴力 或 不 平等 的 遭遇 ［ 　 详细 　 ］  与 联合国 妇女 署 搜狐 公益 一起 睁大眼睛 找出 在 个人 和 公共 生活 中 你 能 为 性别 平等 所 做 的 点滴 做出\n\nActual Summary:\n澄清 认识 误区 没有 暴力 的 生活 是 所有人 的 权利\n\nCreated Summary:\n中国 中国 ２ ２ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n捐款 使用 的 结果 捐款人 不 知道 这是 一个 反常 的 现象 除非 捐款人 不 愿意 知道 如果 捐款人 要 知道 他 应该 能够 知道 但是 确实 有 很多 捐款人 捐 完款 以后 问用 在 哪里 结果 被 告知 你 捐 的 钱 都 经过 审核 了 被 用 在 了 应该 用 的 地方 所有 的 钱 都 放在 一起 怎么 能够 告诉 你 你 的 钱 用 在 哪里 ［ 详细 ］\n\nActual Summary:\n为 青海 玉树 地震 灾区 的 同胞 募捐 行动\n\nCreated Summary:\n美国 将 将 将 将 将 　 　\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n导读 ２ ０ ０ ９ 年 ５ 月 ３ １ 日 是 世界卫生组织 发起 的 第 ２ ２ 个 世界 无烟日 本次 无烟日 的 主题 是 烟草 健康 警示 口号 是 图形 警示 揭露 烟害 真相 国家 控烟 办公室 主任 杨功焕 女士 将 解读 《 ２ ０ ０ ９ 中国 吸烟 控制 报告 》 并 特邀 临床医学 权威 报告 对 解读 提供 深层 支持  我国 禁止 公共场所 吸烟 立法 现状  我国 目前 还 没有 一部 专门 针对 公共场所 禁止 吸烟 的 法律法规 有关 规定 多 是 出现 在 相关 法律法规 的 某些 条款 或 细则 中 １ ３ 年 来 我国 公共场所 禁止 吸烟 地方 法规 发挥 了 一定 作用 … 　 ［ 　 详细 　 ］  公共场所 禁烟 立法 已成 公众 共识  ２ ０ ０ ２ 年 调查 显示 有\n\nActual Summary:\n包含 规定 的 关于 烟草 使用 所致 健康 危害 和 具体 疾病 的 描述 内容\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n８ 月 ３ 日 清晨 ３ ０ 名 中国 青年 学生 索尼 访日 代表团 成员 结束 了 为期 ８ 天 的 访日之旅 踏上 了 回国 的 旅程 随着 ８ 月 ２ 日 盛大 欢送 晚宴 的 结束 由 中华全国青年联合会 简称 全国青联 与 索尼公司 连续 第四次 举办 的 索尼 海外 学生 交流 计划 Ｓ ｏ ｎ ｙ 　 Ｓ ｔ ｕ ｄ ｅ ｎ ｔ 　 Ｐ ｒ ｏ ｊ ｅ ｃ ｔ 　 Ａ ｂ ｒ ｏ ａ ｄ Ｃ ｈ ｉ ｎ ａ － － － Ｓ Ｓ Ｐ Ａ Ｃ ｈ ｉ ｎ ａ 活动 已 正式 落下 帷幕 ３ ０ 名 优秀 高中生 从 ３ ０ ０ 余名 同学 中 脱颖而出 作为 本次 活动 的 友好使者 于 ７ 月 ２ ７ 日 前往 日本 进行 深入\n\nActual Summary:\n２ ０ ０ ９ 索尼 海外 交流 第四日 精彩 瞬间\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n瑞丽 阳光 基金 成立 于 ２ ０ ０ ８ 年 １ ２ 月 １ ２ 日 是 《 瑞丽 》 杂志社 与 中国扶贫基金会 合作 成立 的 专项基金 专注 于 服务 中国 弱势 儿童 群体  瑞丽 阳光 基金 成立 两年 以来 已 陆续 在 河北 青海 四川 等 省 六个 国家级 贫困地区 启动 了 系列 关爱 儿童 的 援助 计划  《 瑞丽 》 杂志社 作为 一个 负责 任 的 企业 公民 将 有效 利用 瑞丽 阳光 基金 和 媒体 品牌 影响力 为 改善 中国 弱势 儿童 的 生存环境 和 推动 社会 慈善 价值观 的 形成 尽 一份 力量 ［ 详细 ］\n\nActual Summary:\nＩ 　 Ｗ ｉ ｌ ｌ 爱 · 未来 摄影 作品 征集 获奖 名单\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n山东 日照 安康 家园 安置 点 集中 了 来自 灾区 的 ５ ２ ２ 名 孩子 其中 ３ ３ ８ 人为 地震 孤儿 在 儿童节 到来之际 为了 让 更 多 的 爱心 人士 帮助 他们 度过 一个 愉快 的 六一 儿童 少年 基金会 与 搜狐 公益 搜狐 社区 开展 因为 有 你 爱 驻 我家 — — 六一 娃娃 礼物 计划 ［ 详细 ］\n\nActual Summary:\n因为 有 你 爱 驻 我家 　 － 　 六一 娃娃 礼物 计划\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n济慈 之 家 小朋友 感受 爱心 椅子  ２ ０ １ ２ 年 ５ 月 １ １ 日 思源 焦点 公益 基金 向 盲童 孤儿院 济慈 之 家 提供 了 首笔 物资 捐赠 这笔 价值 近 万元 的 物资 为曲美 家具 向 思源 · 焦点 公益 基金 提供 的 儿童 休闲椅 将 用于 济慈 之家 的 小孩子 们 日常 使用  本次 受捐 物资 的 济慈 之 家 则 是 一所 专门 收养 中国 盲童 孤儿 并 提供 免费 教育 和 职业培训 的 非盈利性 机构  曲美 家具 代表 粟凡 接受 捐赠 证书  在 当天 捐助 仪式 上 曲美 家具 代表 粟凡 思源 焦点 公益 基金 管委会 副 主任 钱 真 表达 了 对 福利事业 和 弱势群体 的 关注 也 呼吁 全 社会 对 中国 福利事业 给予 更 多 的 关注 和 实际 的\n\nActual Summary:\n思源 焦点 公益 基金 联手 曲美 家具 共 献爱心\n\nCreated Summary:\n美国 将 将 在 在 在 在 在 在\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n孙冕 捐款 发票  给 孩子 送双 运动鞋 公益 行动 由 中华 少年儿童 慈善 救助 基金会 崔永元 我 的 长征 团队 及 搜狐 联合 发起 旨在 帮助 贫困地区 尤指 中西部 贫困山区 穿鞋 难 的 孩子 得到 一双 运动鞋 让 孩子 们 快乐 奔跑 健康成长 活动 得到 了 社会各界 名人 的 响应 和 支持  ６ 月 ７ 日 孙冕 向 给 孩子 送双 运动鞋 项目 捐款 ６ ７ ６ １ ． ２ ０ 元 可以 让 ２ ０ ０ 个 孩子 穿 上 运动鞋 让 他们 快乐 奔跑 健康成长\n\nActual Summary:\n孙冕 向 给 孩子 送双 运动鞋 捐赠 ２ ０ ０ 双 运动鞋\n\nCreated Summary:\n美国 将 将 将 将 将 将 将 将 将\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n中新网 杭州 ６ 月 １ ９ 日电 记者 　 夏毅 今年 １ ０ 月 杭州 地铁 一号线 即将 开通 目前 已 进入 倒计时 阶段 ６ 月 １ ９ 日 杭州市 交通运输 局 召开 《 地铁 乘客 守则 》 市民 沟通 会 参与 沟通 的 有 杭州 交通局 相关 负责人 市民 代表 及 专家 但 本该 出席 的 地铁 集团 相关 负责人 最终 却 未 到 会  现场 有 知情 人士 称 地铁 集团 代表 方走 到 会议室 门口 探 了 探 脑袋 不知 什么 原因 转身 就 离开 了  会后 地铁 集团 回应 媒体 称 他们 不是 牵头 单位 交通局 通知 他们 不是 新闻 发布会 是 内部 会议  记者 从 今天 的 沟通 会 了解 到 关于 《 守则 》 中 市民 反响强烈\n\nActual Summary:\n杭州 地铁 将 出 守则 拟禁 饮食 　 允许 导盲犬 入 车厢\n\nCreated Summary:\n美国 将 在 在 在 被 被 被 被 被 被\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n搜狐 公益 与 中国 儿基会 北京 交广 京华 时报 联合 发起 你 写 故事 我 送 书包 活动 共 捐赠 １ ０ ０ ０ ０ 个 新 书包 给 贫困 新生 ［ 详细 ］\n\nActual Summary:\n为 青海 贫困地区 儿童 捐赠 人生 第一个 书包\n\nCreated Summary:\n视频 － － ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n一波未平 一波又起 据 报道 ７ 月初 人力资源 和 社会保障部 研究所 所长 何平 提出 我国 应 逐步 延长 退休年龄 至 ６ ５ 岁 就 在 这 一 提议 在 社会 上传 得 沸沸扬扬 并 引起 普通职工 强烈 反弹 时 有关 部门 紧急 辟谣 随后 相关 人士 透露 目前 人 社部 关于 延迟 退休 的 研究 重点 主要 侧重于 阶梯式 退休  不少 网民 指出 延迟 退休 引发 热议 的 背后 凸显 民众 积蓄 已久 的 养老 焦虑 以及 对 养老金 巨大 缺口 难以 弥补 的 担忧 延迟 退休 绝非 小事 推出 时机 也 远 未成熟 官方 宜 进一步 深入研究 广泛 征求 民意 而 最 重要 的 是 要 破除 养老 制度 方面 的 种种 不公 否则 有关 延迟 退休 的 纠结 难以 消除  延迟 退休\n\nActual Summary:\n延迟 退休 话题 升温 　 网民 担心 冲击 年轻人 就业\n\nCreated Summary:\n专家 称 称 称 称 称 称 称 称 称\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n空账 倒逼 养老金 双轨制 改革  养老保险 空账 正在 以 ２ ５ ％ 左右 的 速度 扩大 据 《 经济 参考报 》 报道 随着 人口老龄化 日益加剧 我国 养老保险 账户 的 缺口 成 了 人们 普遍 关注 的 问题 根据 社会科学院 世界 社保 研究 中心 主任 郑秉文 给出 的 数字 ２ ０ １ １ 年 城镇 基本 养老保险 个人账户 空账 已经 超过 ２ 　 ． ２ 万亿元 较 ２ ０ １ ０ 年 增加 约 ５ ０ ０ ０ 亿元  对此 不少 网民 指出 虽然 有关 养老金 缺口 的 说法 各异 但 缺口 日益 加大 这一 共识 已 不容 回避 养老金 缺口 形成 有 因 并非 只是 人口老龄化 的 问题 填补 缺口 须 多管齐下 与 备受 争议 的 延迟 退休 相比 破除 积弊已久 的 养老金 双轨制 显得 更为\n\nActual Summary:\n养老保险 空账 正 迅速 扩大 　 养老金 缺口 凸显 老龄化\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n年 审 报告 表明 该 基金会 ２ ０ ０ ８ 年度 公益事业 支出 约 为 ３ ０ ． ６ 万元 而 上年 总收入 为 ２ ３ ３ 万多元 仅 占 上年 总收入 的 １ ３ ． １ ％ 而 按照 国务院 《 基金会 管理条例 》 规定 公募 性 基金会 公益事业 支出 应 占 上 年度 总收入 的 ７ ０ ％ 以上 两者 相比 基金会 公益性 支出 竟然 少 了 ５ ７ ％ ［ 详细 ］  基金会 募集 资金 很 困难 有时 为了 募集 两三百元 说不定 要 跑 上 好几趟 策划 公司 有 其 优势 于是 就 与 公司 合作 再说 市场化 操作 并 不 违反 相关 规定 募集 费用 是 高 了 些 但 法律法规 并 没有 对此 有 明确规定 ［ 详细 ］  基金会 的 几个 头头 竟 背着 大多数 副理事长\n\nActual Summary:\n１ ５ ０ 年来 杜南 的 个人 有 能力 改变 世界 的 信念 激励 着 全世界 上 亿 志愿者 … ［\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n２ ０ １ ２ 年 ７ 月 １ ８ 日 北京 讯 　 现代 传播 携手 联合国开发计划署 Ｕ Ｎ Ｄ Ｐ 发布 公益 短片 《 ２ ０ ３ ２ 我们 期望 的 未来 》 表达 中国 民众 对 未来 的 愿景 联合国 秘书长 潘基文 先生 联合国 驻华 协调员 联合国开发计划署 驻华 代表 罗黛琳 女士 出席 了 活动 并 对 现代 传播 为此 公益 项目 做出 的 努力 表示 嘉许 和 感谢  公益 短片 中 ３ ２ 位 来自 各地 的 中国 民众 表达 了 自己 的 未来 愿景 该片 在 里 约 ２ ０ ＋ 峰会 期间 播出 包括 各国 领导人 在内 的 全球 公众 听到 了 来自 中国 的 强有力 的 声音 联合国开发计划署 中国 亲善大使 著名演员 周迅 支持 并 参与 拍摄 周迅 表示 我 相信 每 一个 人 的 愿景 和 声音\n\nActual Summary:\n公益 短片 《 ２ ０ ３ ２ 我们 期望 的 未来 》 表达 未来 愿景\n\nCreated Summary:\n伦敦 称 称 称 称 称 称 称 称 称 称\n\n\n\n\n\n ----------------------------------------------------------------------------------------------------\nActual Text:\n操作 不当 致 ２ ０ 吨 油 污水 泄漏  本报 见习 记者 刘帅 本报记者 刘红杰  ７ 月 ２ １ 日蓝星 石油 有限公司 济南 分公司 简称 蓝星 石油 发生 漏油 事件 后 小清河 及其 支流 梁王河 又称 石河 出现 大面积 油污 沿岸 很多 村民 闻后 出现 胸闷 气短 等 不适 症状 ２ ３ 日 济南市 环保局 通报 该 事故 的 处置 情况 称 漏油 因 员工 操作 不当 造成 ２ ０ 吨 油 污水 泄漏 所 含 油污 大部分 为 柴油 有 少量 汽油 通过 检测 油 污水 未 对 小清河 干流 水质 造成 明显 影响  ２ ３ 日 济南市 环保局 通报 蓝星 石油 含油 污水 泄漏 事件 处置 情况 事故 因 员工 操作 不当 造成 蓝星 石油 ２ ０ 吨 含油 污水 泄漏 石河 所 含\n\nActual Summary:\n石油 公司 操作 不当 致 ２ ０ 吨 污水 泄漏 　 村民 闻后 胸闷\n\nCreated Summary:\n１ １ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０ ０\n\n\n"
    }
   ],
   "source": [
    "# show results\n",
    "summarizer_model_utils.sample_results(preds,\n",
    "                                      ind2word,\n",
    "                                      word2ind,\n",
    "                                      converted_summaries[:50],\n",
    "                                      converted_texts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_bcG5CPmw2m"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Generally I am really impressed by how well the model works. \n",
    "We only used a limited amount of data, trained it for a limited amount of time and used nearly random hyperparameters and it still delivers good results. \n",
    "\n",
    "However, we are clearly overfitting the training data and the model does not perfectly generalize.\n",
    "Sometimes the summaries the model creates are good, sometimes bad, sometimes they are better than the original ones and sometimes they are just really funny.\n",
    "\n",
    "\n",
    "Therefore it would be really interesting to scale it up and see how it performs. \n",
    "\n",
    "To sum up, I am impressed by seq2seq models, they perform great on many different tasks and I look foward to exploring more possible applications. \n",
    "(speech recognition...)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "summarizer_amazon_reviews.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python_defaultSpec_1599901174589"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}